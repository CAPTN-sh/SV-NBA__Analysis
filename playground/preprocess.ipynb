{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Preprocess](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the instructions for preprocessing the data. The preprocessing steps are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "1. [Preprocess](#toc1_)    \n",
    "1.1. [Dependencies and paths](#toc1_1_)    \n",
    "1.2. [Load the pickled data](#toc1_2_)    \n",
    "1.3. [Check which pickled files are corrupted](#toc1_3_)    \n",
    "1.4. [Split the data into train, validate and test sets based on the seasonality](#toc1_4_)    \n",
    "1.5. [(intermediate-) Save the train, validate and test ``Dataset`` objects as pickled files](#toc1_5_)    \n",
    "1.6. [Extend the feature set with some temoporal features](#toc1_6_)    \n",
    "1.7. [(intermediate-) Save the extended train, validate and test ``pd.DataFrames`` objects as pickled files](#toc1_7_)    \n",
    "1.8. [Explore the train, validate and test sets](#toc1_8_)    \n",
    "1.9. [Remove Stops, Clean all NaN rows, and Downscale the datasets](#toc1_9_)    \n",
    "1.10. [Scale the train and validate sets. Save the scaler as a pickled file](#toc1_10_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=true\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. <a id='toc1_1_'></a>[Dependencies and paths](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEPENDENCIES >>>\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add root directory to path for imports >\n",
    "root_dir = Path.cwd().resolve().parent\n",
    "if root_dir.exists():\n",
    "    sys.path.append(str(root_dir))\n",
    "else:\n",
    "    raise FileNotFoundError('Root directory not found')\n",
    "\n",
    "# import custom libraries >\n",
    "from src.load import load_multiple_trajectoryCollection_parallel_pickle as lmtp\n",
    "from src.load import load_df_to_dataset\n",
    "from src.traj_dataloader import (separate_files_by_season, \n",
    "                                 split_data, \n",
    "                                 get_files,\n",
    "                                 AISDataset,\n",
    "                                 )\n",
    "\n",
    "import dotsi\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Plot >\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scienceplots  # https://github.com/garrettj403/SciencePlots?tab=readme-ov-file\n",
    "plt.style.use(['science', 'grid', 'notebook'])  # , 'ieee'\n",
    "\n",
    "# %matplotlib inline\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FLAGS & GLOBAL VALUES >>>\n",
    "# Key for lines used for exploration and not for execution\n",
    "EXPLORE = False\n",
    "\n",
    "# Key for debugging\n",
    "DEBUG = False\n",
    "\n",
    "# Key if features are not divided into train, validate and test and saved as pickles\n",
    "SPLIT_TVT_DATA = False\n",
    "\n",
    "# Extend TVT data with temporal featues\n",
    "EXTEND_TVT_DATA = False\n",
    "\n",
    "# Remove Anchoring Ships\n",
    "REMOVE_STOP = False\n",
    "\n",
    "# Down sample the resolution\n",
    "DOWN_SAMPLE = True  # used with SCALE and SAVE_SCALE to save the scaled data: (if True) with down sampled resolution, or with (not False) not.\n",
    "\n",
    "# Drop corrupted features  # See corrupted_features list\n",
    "DROP_CORRUPTED_FEATURES = True\n",
    "\n",
    "# Scale TVT data\n",
    "SCALE = True\n",
    "\n",
    "# seed\n",
    "split_seed = 42\n",
    "\n",
    "# If DOWN_SAMPLE, define the target time resolution\n",
    "targeted_resolution_min = 1  # minute\n",
    "\n",
    "# TODO: The following featues are corrupted by containing NaNs. Fix this. For now, these columns are dropped\n",
    "corrupted_features = [\"stopped\", \"abs_ccs\", \"curv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PATHS >>>\n",
    "# data dir\n",
    "data_dir = root_dir / 'data'\n",
    "data_dir = data_dir.resolve()\n",
    "if not data_dir.exists():\n",
    "    raise FileNotFoundError('Data directory not found')\n",
    "\n",
    "# aistraj dir\n",
    "assets_dir = root_dir / 'data' / 'local' / 'aistraj'\n",
    "assets_dir = assets_dir.resolve()\n",
    "if not assets_dir.exists():\n",
    "    raise FileNotFoundError('Assets directory not found')\n",
    "\n",
    "# models dir\n",
    "models_dir = root_dir / 'models'\n",
    "models_dir = models_dir.resolve()\n",
    "if not models_dir.exists():\n",
    "    raise FileNotFoundError('Models directory not found')\n",
    "\n",
    "# raw-pickle dir\n",
    "raw_pickle_dir = assets_dir / 'pickle'\n",
    "raw_pickle_dir = raw_pickle_dir.resolve()\n",
    "if not raw_pickle_dir.exists():\n",
    "    raise FileNotFoundError('Pickle Assets directory not found')\n",
    "\n",
    "# train-validate-test (tvt) dir\n",
    "tvt_assets_dir = assets_dir / 'tvt_assets'\n",
    "tvt_assets_dir = tvt_assets_dir.resolve()\n",
    "if not tvt_assets_dir.exists():\n",
    "    raise FileNotFoundError('Train-Validate-Test Assets directory not found')\n",
    "\n",
    "# tvt: original pickle dir\n",
    "tvt_original_dir = tvt_assets_dir / 'original'\n",
    "tvt_original_dir = tvt_original_dir.resolve()\n",
    "if not tvt_original_dir.exists():\n",
    "    raise FileNotFoundError('TVT Original Pickled Data directory not found')\n",
    "\n",
    "# tvt: extended pickle dir\n",
    "tvt_extended_dir = tvt_assets_dir / 'extended'\n",
    "tvt_extended_dir = tvt_extended_dir.resolve()\n",
    "if not tvt_extended_dir.exists():\n",
    "    raise FileNotFoundError('TVT Extended Pickled Data directory not found')\n",
    "\n",
    "# tvt: scaled pickle dir\n",
    "tvt_scaled_dir = tvt_assets_dir / 'scaled'\n",
    "tvt_scaled_dir = tvt_scaled_dir.resolve()\n",
    "if not tvt_scaled_dir.exists():\n",
    "    raise FileNotFoundError('TVT Scaled Pickled Data directory not found')\n",
    "\n",
    "# tvt: logs dir\n",
    "tvt_logs_dir = tvt_assets_dir / 'logs'\n",
    "tvt_logs_dir = tvt_logs_dir.resolve()\n",
    "if not tvt_logs_dir.exists():\n",
    "    raise FileNotFoundError('TVT logs directory not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. <a id='toc1_2_'></a>[Load the pickled data](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Get list of the pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPLIT_TVT_DATA:\n",
    "    pickle_files_lst = get_files(str(raw_pickle_dir), extension='.pickle')  # get_files(str(test_pickle_assets_dir), extension='.pickle')\n",
    "    print(pickle_files_lst)\n",
    "    print(len(pickle_files_lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Get the absolute path and ckeck if the absolute path is recognised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPLIT_TVT_DATA:\n",
    "    if pickle_files_lst:\n",
    "        absolute_paths = [os.path.join(raw_pickle_dir, file) for file in pickle_files_lst]\n",
    "        existing_files = [file for file in absolute_paths if os.path.exists(file)]\n",
    "        print(\"Number of Existing files:\", len(existing_files))\n",
    "        non_existing_files = [file for file in absolute_paths if not os.path.exists(file)]\n",
    "        print(\"Existing files:\", existing_files)\n",
    "        print(\"Non-existing files:\", non_existing_files)\n",
    "    else:\n",
    "        print(\"pickle_files_lst is empty\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. <a id='toc1_3_'></a>[Check which pickled files are corrupted](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one pickle was corrupted and it broke the loop, I will check which pickle files are corrupted and remove them from the list of pickles to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#> As one pickle is corrupted, we need to load the pickle files and check if they are valid\n",
    "if DEBUG:\n",
    "    from src.load import load_multiple_trajectoryCollection_pickle as lp\n",
    "    for file in pickle_files_lst:\n",
    "        print(f'loading {file}: ', end=' ')\n",
    "        try:\n",
    "            abs_file = os.path.join(raw_pickle_dir, file)\n",
    "            df = lp(abs_file)\n",
    "            print('success')\n",
    "            del df\n",
    "        except Exception as e:\n",
    "            print('failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. <a id='toc1_4_'></a>[Split the data into train, validate and test sets based on the seasonality](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Split the file names based on the seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPLIT_TVT_DATA:\n",
    "    summer_files, autumn_files, winter_files, spring_files = separate_files_by_season(pickle_files_lst)\n",
    "    all_files_by_season = {'summer': summer_files, 'autumn': autumn_files, 'winter': winter_files, 'spring': spring_files}\n",
    "\n",
    "    for season_key, season_values in all_files_by_season.items():\n",
    "        print(season_key, len(season_values))\n",
    "        if season_values:\n",
    "            print(season_values)  \n",
    "        print(\"==\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Create the files list for train, validate and test. For each season, split the data to train (64%), validate (16%) and test (20%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPLIT_TVT_DATA:\n",
    "    train_files_lst, validate_files_lst, test_files_lst = [], [], []\n",
    "\n",
    "    for season_key, season_values in all_files_by_season.items():\n",
    "        if season_values:  # If the list is not empty\n",
    "            train_lst, validate_lst, test_lst = split_data(season_values, \n",
    "                                                           test_percent=0.2, \n",
    "                                                           validate_percent=0.2, \n",
    "                                                           random_seed=split_seed)\n",
    "            train_files_lst.extend(train_lst)\n",
    "            validate_files_lst.extend(validate_lst)\n",
    "            test_files_lst.extend(test_lst)\n",
    "            \n",
    "            #> print for debug\n",
    "            print(season_key, len(season_values))\n",
    "            print(\"Train files:\", len(train_lst))\n",
    "            print(\"Validate files:\", len(validate_lst))\n",
    "            print(\"Test files:\", len(test_lst))\n",
    "            print(\"==\"*20)\n",
    "        else:  # List is empty\n",
    "            print(f\"{season_key} is empty\")\n",
    "            print(\"==\"*20)\n",
    "        \n",
    "    print(\"len train files:\", len(train_files_lst))\n",
    "    print(\"len validate files:\", len(validate_files_lst))\n",
    "    print(\"len test files:\", len(test_files_lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Create the ``torch.utils.data.Dataset`` objects for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Dataset object\n",
    "if SPLIT_TVT_DATA:\n",
    "    validate_dataset = AISDataset(validate_files_lst, \n",
    "                                  raw_pickle_dir, \n",
    "                                  transform=None, \n",
    "                                  num_workers=16)\n",
    "    print(len(validate_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset object\n",
    "if SPLIT_TVT_DATA:\n",
    "    test_dataset = AISDataset(test_files_lst, \n",
    "                              raw_pickle_dir, \n",
    "                              transform=None, \n",
    "                              num_workers=16)\n",
    "    print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataset object\n",
    "if SPLIT_TVT_DATA:\n",
    "    train_dataset = AISDataset(train_files_lst, \n",
    "                               raw_pickle_dir,\n",
    "                               transform=None, \n",
    "                               num_workers=16)\n",
    "    print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. <a id='toc1_5_'></a>[(intermediate-) Save the train, validate and test ``Dataset`` objects as pickled files](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Save the ``Dataset`` objects as pickled files in ``tvt_original_dir``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Train >\n",
    "if SPLIT_TVT_DATA:\n",
    "    with open(tvt_original_dir / 'train_dataset.pickle', 'wb') as f:\n",
    "        pickle.dump(train_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Test >\n",
    "if SPLIT_TVT_DATA:\n",
    "    with open(tvt_original_dir / 'test_dataset.pickle', 'wb') as f:\n",
    "        pickle.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Train >\n",
    "if SPLIT_TVT_DATA:\n",
    "    with open(tvt_original_dir / 'validate_dataset.pickle', 'wb') as f:\n",
    "        pickle.dump(validate_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ For logging and repeatedity in experiments. Save the train, validate, and test file names as txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPLIT_TVT_DATA:\n",
    "    dataset = dotsi.Dict({'train': train_files_lst, \n",
    "                       'validate': validate_files_lst, \n",
    "                       'test': test_files_lst})\n",
    "    \n",
    "    for set in dataset:\n",
    "        files_txt = \"\\n\".join(dataset[set])\n",
    "        files_txt = files_txt.replace(\" \", \"\\n\")  # Replace spaces with new lines\n",
    "        with open(tvt_logs_dir / f'{set}_files.txt', 'w') as f:\n",
    "            f.write(files_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. <a id='toc1_6_'></a>[Extend the feature set with some temoporal features](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Define a function to get the season from the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_season(date):\n",
    "    summer_start_date = 620\n",
    "    autumn_start_date = 922\n",
    "    winter_start_date = 1221\n",
    "    spring_start_date = 320\n",
    "    \n",
    "    file_month_day = int(date.strftime('%m%d'))\n",
    "    \n",
    "    if spring_start_date <= file_month_day < summer_start_date:\n",
    "        return 0  # Spring\n",
    "    elif summer_start_date <= file_month_day < autumn_start_date:\n",
    "        return 1  # Summer\n",
    "    elif autumn_start_date <= file_month_day < winter_start_date:\n",
    "        return 2  # Autumn\n",
    "    else:\n",
    "        return 3  # Winter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Add the temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXTEND_TVT_DATA:\n",
    "    for df in [train_df, validate_df, test_df]:\n",
    "        df['season'] = df['datetime'].apply(assign_season)\n",
    "        # Assign part of the day based on hour using vectorized operations\n",
    "        df['part_of_day'] = np.where((df['datetime'].dt.hour >= 6) & (df['datetime'].dt.hour <= 12), 0, \n",
    "                        np.where((df['datetime'].dt.hour >= 13) & (df['datetime'].dt.hour <= 18), 1, 2))\n",
    "        # Encode hours, minutes, and seconds using sine and cosine to capture cyclical nature\n",
    "        df['month_sin'] = np.sin(df['datetime'].dt.month * (2. * np.pi / 12))\n",
    "        df['month_cos'] = np.cos(df['datetime'].dt.month * (2. * np.pi / 12))\n",
    "        df['hour_sin'] = np.sin(df['datetime'].dt.hour * (2. * np.pi / 24))\n",
    "        df['hour_cos'] = np.cos(df['datetime'].dt.hour * (2. * np.pi / 24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. <a id='toc1_7_'></a>[(intermediate-) Save the extended train, validate and test ``pd.DataFrames`` objects as pickled files](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Train df >\n",
    "if EXTEND_TVT_DATA:\n",
    "    with open(tvt_extended_dir / 'extend_train_df.pickle', 'wb') as f:\n",
    "        pickle.dump(train_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Validate df >\n",
    "if EXTEND_TVT_DATA:\n",
    "    with open(tvt_extended_dir / 'extend_validate_df.pickle', 'wb') as f:\n",
    "        pickle.dump(validate_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Test df >\n",
    "if EXTEND_TVT_DATA:\n",
    "    with open(tvt_extended_dir / 'extend_test_df.pickle', 'wb') as f:\n",
    "        pickle.dump(test_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8. <a id='toc1_8_'></a>[Explore the train, validate and test sets](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    train_df = dataset.train.data\n",
    "    validate_df = dataset.validate.data\n",
    "    test_df = dataset.test.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    train_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    validate_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    validate_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    validate_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    test_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: </br> The following columns has lot of corrupted values (stored as NaN). Therefore, before any further processing, drop (using ``*_df.drop(['col1', 'col2'], axis=1)``) these columns from the data:\n",
    "> + ``stopped``\n",
    "> + ``abs_ccs``\n",
    "> + ``curv``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9. <a id='toc1_9_'></a>[Remove Stops, Clean all NaN rows, and Downscale the datasets](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_paths = {\n",
    "                'train': tvt_extended_dir / 'extend_train_df.pickle',\n",
    "                'validate': tvt_extended_dir / 'extend_validate_df.pickle',\n",
    "                'test': tvt_extended_dir / 'extend_test_df.pickle'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for k, v in target_paths.items():\n",
    "    # Load the dataset >\n",
    "    asset_df = pd.DataFrame()\n",
    "    print(f\"Loading {k} dataset\")\n",
    "    asset_df = load_df_to_dataset(v).data\n",
    "    original_sample_cnt = asset_df.shape[0]\n",
    "    print(f'\\t\\t>> sample count: {original_sample_cnt}')\n",
    "    \n",
    "    # Drop the NaN values. Ignore datetime value >\n",
    "    print(f\"\\t> Dropping row with all nan values\")\n",
    "    asset_df = asset_df.dropna(subset= asset_df.columns.difference(['datetime', 'epoch']), how='all')\n",
    "    new_sample_cnt = asset_df.shape[0]\n",
    "    print(f'\\t\\t>> number of dropped samples due to rows of nan: {original_sample_cnt - new_sample_cnt}')\n",
    "    last_sample_cnt = new_sample_cnt\n",
    "        \n",
    "    if DEBUG:  # Check if there are any NaN values\n",
    "        print(f'\\t\\t>> counting NaN values in each column')\n",
    "        nan_counts = asset_df.isna().sum()\n",
    "        for column, value in nan_counts.items():\n",
    "            print(f'\\t\\t\\t>>> {column}: {value}')\n",
    "    \n",
    "    if REMOVE_STOP:  # Drop the stopped samples >\n",
    "        print(f\"\\t> Dropping stopped samples\")\n",
    "        asset_df = asset_df[asset_df['stopped'] != 1]\n",
    "        new_sample_cnt = asset_df.shape[0]\n",
    "        print(f'\\t\\t>> number of dropped samples due to stopping: {last_sample_cnt - new_sample_cnt}')\n",
    "        last_sample_cnt = new_sample_cnt\n",
    "        \n",
    "        if DEBUG:  # Check if there are any NaN values\n",
    "            print(f'\\t\\t>> counting NaN values in each column')\n",
    "            nan_counts = asset_df.isna().sum()\n",
    "            for column, value in nan_counts.items():\n",
    "                print(f'\\t\\t\\t>>> {column}: {value}')\n",
    "            \n",
    "    if DOWN_SAMPLE:  # Reduce the resolution >\n",
    "        print(f\"\\t> Downsampling for the resolution of {targeted_resolution_min} min\")\n",
    "        asset_df = reduce_resolution(asset_df, resolution_minutes=targeted_resolution_min)\n",
    "        \n",
    "        # TODO: For some reason,t he result df contains alot of missing values rows. This needs to be fixed. For now it is dropped\n",
    "        asset_df = asset_df.dropna(subset= asset_df.columns.difference(['datetime', 'epoch']), how='all')\n",
    "        \n",
    "        new_sample_cnt = asset_df.shape[0]\n",
    "        print(f'\\t\\t>> number of dropped samples due to downsampling the resolution for {targeted_resolution_min} min.: {last_sample_cnt - new_sample_cnt}')\n",
    "        last_sample_cnt = new_sample_cnt\n",
    "        \n",
    "        if DEBUG:  # Check if there are any NaN values\n",
    "            print(f'\\t\\t>> counting NaN values in each column')\n",
    "            nan_counts = asset_df.isna().sum()\n",
    "            for column, value in nan_counts.items():\n",
    "                print(f'\\t\\t\\t>>> {column}: {value}')\n",
    "                \n",
    "    print(f'\\t> the final sample count: {asset_df.shape[0]}')\n",
    "    print(f'\\t\\t>> counting NaN values in each column')\n",
    "    nan_counts = asset_df.isna().sum()\n",
    "    for column, value in nan_counts.items():\n",
    "        print(f'\\t\\t\\t>>> {column}: {value}')\n",
    "    \n",
    "    # Save the extended_cleaned df as parquet >\n",
    "    print('Saving >>>', end=' ')\n",
    "    if DOWN_SAMPLE:\n",
    "        asset_df.to_parquet(tvt_extended_dir / f'cleaned_downsampled_extended_{k}_df.parquet')\n",
    "    else:\n",
    "        asset_df.to_parquet(tvt_extended_dir / f'cleaned_extended_{k}_df.parquet')\n",
    "    print('success')\n",
    "    del asset_df\n",
    "    print('=='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10. <a id='toc1_10_'></a>[Scale the train and validate sets. Save the scaler as a pickled file](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the cleaned datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_paths = {'train': None, 'validate': None, 'test': None}\n",
    "\n",
    "if DOWN_SAMPLE:\n",
    "    import_paths = {\n",
    "                    'train': tvt_extended_dir / 'cleaned_downsampled_extended_train_df.parquet',\n",
    "                    'validate': tvt_extended_dir / 'cleaned_downsampled_extended_validate_df.parquet',\n",
    "                    'test': tvt_extended_dir / 'cleaned_downsampled_extended_test_df.parquet',\n",
    "                    }\n",
    "else:\n",
    "    import_paths = {\n",
    "                    'train': tvt_extended_dir / 'cleaned_extended_train_df.parquet',\n",
    "                    'validate': tvt_extended_dir / 'cleaned_extended_validate_df.parquet',\n",
    "                    'test': tvt_extended_dir / 'cleaned_extended_test_df.parquet',\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Declare the custom scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30 µs, sys: 36 µs, total: 66 µs\n",
      "Wall time: 76.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if SCALE:\n",
    "    # Split the column names into groups for scaling each with different scaler >\n",
    "    cols_not_to_scale = ['epoch', 'datetime', 'obj_id', 'traj_id', 'month_sin', 'month_cos', 'hour_sin', 'hour_cos', 'season', 'part_of_day', 'stopped', 'abs_ccs', 'curv']\n",
    "    cols_to_standard_scale = ['aad', 'cdd', 'dir_ccs'] \n",
    "    angles_to_min_max_scale = ['cog_c', 'rot_c'] # min=0, max=360\n",
    "    distances_to_min_max_scale = ['distance_c', 'dist_ww', 'dist_ra', 'dist_cl', 'dist_ma']\n",
    "    cols_to_robust_scale = ['speed_c', 'acc_c']\n",
    "    location_min_max_scale = ['lon', 'lat']\n",
    "\n",
    "    columns_in_order = cols_not_to_scale + cols_to_standard_scale + angles_to_min_max_scale + distances_to_min_max_scale + cols_to_robust_scale + location_min_max_scale\n",
    "\n",
    "    # Define the scaler of each group >\n",
    "    transformers = [('no_scaler', 'passthrough', cols_not_to_scale),\n",
    "                    ('standard_scaler', StandardScaler(), cols_to_standard_scale),\n",
    "                    ('angles_min_max_scaler', CustomMinMaxScaler(feature_range=(0,1), min=0, max=360), angles_to_min_max_scale),\n",
    "                    ('distances_min_max_scaler', CustomMinMaxScaler(feature_range=(0,1), min=0), distances_to_min_max_scale),\n",
    "                    ('robust_scaler', RobustScaler(), cols_to_robust_scale),\n",
    "                    ('location_min_max_scale', MinMaxScaler(), location_min_max_scale)]\n",
    "\n",
    "    # Create column transformer within a pipeline >\n",
    "    scale_transformer =  ColumnTransformer(transformers, remainder='passthrough', n_jobs=10) # passthrough: None mentioned columns shall remain as it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 904 ms, sys: 2.28 s, total: 3.19 s\n",
      "Wall time: 2.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if SCALE:\n",
    "    # Fit the scaler on train and validate data >\n",
    "    train_df = pd.read_parquet(import_paths['train'])  # Load the train dataset\n",
    "        \n",
    "    validate_df = pd.read_parquet(import_paths['validate'])  # Load the validate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>stopped</th>\n",
       "      <th>cog_c</th>\n",
       "      <th>aad</th>\n",
       "      <th>rot_c</th>\n",
       "      <th>speed_c</th>\n",
       "      <th>distance_c</th>\n",
       "      <th>acc_c</th>\n",
       "      <th>cdd</th>\n",
       "      <th>abs_ccs</th>\n",
       "      <th>...</th>\n",
       "      <th>traj_id</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>obj_id</th>\n",
       "      <th>season</th>\n",
       "      <th>part_of_day</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-03-24 13:13:00</th>\n",
       "      <td>1648127580</td>\n",
       "      <td>0</td>\n",
       "      <td>99.080888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.587292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10.139315</td>\n",
       "      <td>54.36597</td>\n",
       "      <td>209221000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-24 13:14:00</th>\n",
       "      <td>1648127640</td>\n",
       "      <td>0</td>\n",
       "      <td>104.065234</td>\n",
       "      <td>1.471039</td>\n",
       "      <td>0.147104</td>\n",
       "      <td>0.5063</td>\n",
       "      <td>5.063</td>\n",
       "      <td>-0.001443</td>\n",
       "      <td>33.386957</td>\n",
       "      <td>0.10741</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10.139818</td>\n",
       "      <td>54.36591</td>\n",
       "      <td>209221000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-24 13:15:00</th>\n",
       "      <td>1648127700</td>\n",
       "      <td>0</td>\n",
       "      <td>95.703471</td>\n",
       "      <td>2.762937</td>\n",
       "      <td>-0.276294</td>\n",
       "      <td>0.201057</td>\n",
       "      <td>2.010571</td>\n",
       "      <td>-0.003743</td>\n",
       "      <td>53.728167</td>\n",
       "      <td>0.183571</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10.140124</td>\n",
       "      <td>54.365873</td>\n",
       "      <td>209221000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-24 13:16:00</th>\n",
       "      <td>1648127760</td>\n",
       "      <td>0</td>\n",
       "      <td>163.765402</td>\n",
       "      <td>145.052777</td>\n",
       "      <td>14.505278</td>\n",
       "      <td>0.506937</td>\n",
       "      <td>5.069375</td>\n",
       "      <td>0.040647</td>\n",
       "      <td>65.10097</td>\n",
       "      <td>1.770759</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10.140201</td>\n",
       "      <td>54.365856</td>\n",
       "      <td>209221000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-24 13:17:00</th>\n",
       "      <td>1648127820</td>\n",
       "      <td>0</td>\n",
       "      <td>89.999999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013426</td>\n",
       "      <td>0.134263</td>\n",
       "      <td>-0.000739</td>\n",
       "      <td>68.432622</td>\n",
       "      <td>2.399117</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10.140222</td>\n",
       "      <td>54.365877</td>\n",
       "      <td>209221000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          epoch  stopped       cog_c         aad      rot_c  \\\n",
       "datetime                                                                      \n",
       "2022-03-24 13:13:00  1648127580        0   99.080888         0.0        0.0   \n",
       "2022-03-24 13:14:00  1648127640        0  104.065234    1.471039   0.147104   \n",
       "2022-03-24 13:15:00  1648127700        0   95.703471    2.762937  -0.276294   \n",
       "2022-03-24 13:16:00  1648127760        0  163.765402  145.052777  14.505278   \n",
       "2022-03-24 13:17:00  1648127820        0   89.999999         0.0        0.0   \n",
       "\n",
       "                      speed_c  distance_c     acc_c        cdd   abs_ccs  ...  \\\n",
       "datetime                                                                  ...   \n",
       "2022-03-24 13:13:00  0.587292         0.0       0.0        0.0       0.0  ...   \n",
       "2022-03-24 13:14:00    0.5063       5.063 -0.001443  33.386957   0.10741  ...   \n",
       "2022-03-24 13:15:00  0.201057    2.010571 -0.003743  53.728167  0.183571  ...   \n",
       "2022-03-24 13:16:00  0.506937    5.069375  0.040647   65.10097  1.770759  ...   \n",
       "2022-03-24 13:17:00  0.013426    0.134263 -0.000739  68.432622  2.399117  ...   \n",
       "\n",
       "                     traj_id        lon        lat       obj_id  season  \\\n",
       "datetime                                                                  \n",
       "2022-03-24 13:13:00        0  10.139315   54.36597  209221000.0     0.0   \n",
       "2022-03-24 13:14:00        0  10.139818   54.36591  209221000.0     0.0   \n",
       "2022-03-24 13:15:00        0  10.140124  54.365873  209221000.0     0.0   \n",
       "2022-03-24 13:16:00        0  10.140201  54.365856  209221000.0     0.0   \n",
       "2022-03-24 13:17:00        0  10.140222  54.365877  209221000.0     0.0   \n",
       "\n",
       "                     part_of_day  month_sin     month_cos  hour_sin  hour_cos  \n",
       "datetime                                                                       \n",
       "2022-03-24 13:13:00          1.0        1.0  6.123234e-17 -0.258819 -0.965926  \n",
       "2022-03-24 13:14:00          1.0        1.0  6.123234e-17 -0.258819 -0.965926  \n",
       "2022-03-24 13:15:00          1.0        1.0  6.123234e-17 -0.258819 -0.965926  \n",
       "2022-03-24 13:16:00          1.0        1.0  6.123234e-17 -0.258819 -0.965926  \n",
       "2022-03-24 13:17:00          1.0        1.0  6.123234e-17 -0.258819 -0.965926  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Concat the train and validate sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 72.4 ms, sys: 129 ms, total: 202 ms\n",
      "Wall time: 200 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if SCALE:\n",
    "    combined_df = pd.concat([train_df, validate_df])  # Combine the train and validate datasets\n",
    "    \n",
    "    # For DOWN_SAMPLE: The datatime is used as index, therefore, it is reset to be a column >\n",
    "    combined_df.reset_index(inplace=True)  # Reset index to move datetime index to a column and create a new numerical index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Fit the scaler on the train and validate sets (not the test!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gfalouji/miniconda3/envs/captn-nba/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:702: FutureWarning: The output of the 'no_scaler' transformer for column 'abs_ccs' has dtype Float64 and uses pandas.NA to represent null values. Storing this output in a numpy array can cause errors in downstream scikit-learn estimators, and inefficiencies. Starting with scikit-learn version 1.6, this will raise a ValueError. To avoid this problem you can (i) store the output in a pandas DataFrame by using ColumnTransformer.set_output(transform='pandas') or (ii) modify the input data or the 'no_scaler' transformer to avoid the presence of pandas.NA (for example by using pandas.DataFrame.astype).\n",
      "  warnings.warn(\n",
      "/home/gfalouji/miniconda3/envs/captn-nba/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:702: FutureWarning: The output of the 'no_scaler' transformer for column 'curv' has dtype Float64 and uses pandas.NA to represent null values. Storing this output in a numpy array can cause errors in downstream scikit-learn estimators, and inefficiencies. Starting with scikit-learn version 1.6, this will raise a ValueError. To avoid this problem you can (i) store the output in a pandas DataFrame by using ColumnTransformer.set_output(transform='pandas') or (ii) modify the input data or the 'no_scaler' transformer to avoid the presence of pandas.NA (for example by using pandas.DataFrame.astype).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.22 s, sys: 4.13 s, total: 10.3 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if SCALE:\n",
    "    scale_transformer.fit(combined_df)  # Fit the scaler on the combined dataset\n",
    "    \n",
    "    # Delete the train, validate and combined dataset to free up the memory >\n",
    "    del combined_df\n",
    "    del train_df\n",
    "    del validate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Save the fitted scaler in ``models_dir``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 1.95 ms, total: 1.95 ms\n",
      "Wall time: 1.75 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if SCALE:\n",
    "    # Define the scaler name >\n",
    "    scaler_name = None\n",
    "    if DOWN_SAMPLE:\n",
    "        scaler_name = 'cleaned_downsampled_extended_scaler.pickle'\n",
    "        column_order_name = 'cleaned_downsampled_columns_after_extended_scale_order.txt'\n",
    "    else:\n",
    "        scaler_name = 'cleaned_extended_scaler.pickle'\n",
    "        column_order_name = 'cleaned_columns_after_scale_extended_order.txt'\n",
    "    \n",
    "    # Save the scaler >\n",
    "    with open(models_dir / scaler_name, 'wb') as f:\n",
    "        pickle.dump(scale_transformer, f)   \n",
    "        \n",
    "    # Save the columns order after scaling >\n",
    "    with open(models_dir / column_order_name, 'w') as f:\n",
    "        f.write(\"\\n\".join(columns_in_order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Scale the train, validate and test sets. Then save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset\n",
      "\t\t>> sample count: 3025515\n",
      "\t> Transforming the train dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gfalouji/miniconda3/envs/captn-nba/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:702: FutureWarning: The output of the 'no_scaler' transformer for column 'abs_ccs' has dtype Float64 and uses pandas.NA to represent null values. Storing this output in a numpy array can cause errors in downstream scikit-learn estimators, and inefficiencies. Starting with scikit-learn version 1.6, this will raise a ValueError. To avoid this problem you can (i) store the output in a pandas DataFrame by using ColumnTransformer.set_output(transform='pandas') or (ii) modify the input data or the 'no_scaler' transformer to avoid the presence of pandas.NA (for example by using pandas.DataFrame.astype).\n",
      "  warnings.warn(\n",
      "/home/gfalouji/miniconda3/envs/captn-nba/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:702: FutureWarning: The output of the 'no_scaler' transformer for column 'curv' has dtype Float64 and uses pandas.NA to represent null values. Storing this output in a numpy array can cause errors in downstream scikit-learn estimators, and inefficiencies. Starting with scikit-learn version 1.6, this will raise a ValueError. To avoid this problem you can (i) store the output in a pandas DataFrame by using ColumnTransformer.set_output(transform='pandas') or (ii) modify the input data or the 'no_scaler' transformer to avoid the presence of pandas.NA (for example by using pandas.DataFrame.astype).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t>> transformation complete\n",
      "\t> Dropping corrupted features\n",
      "\t\t>> number of dropped corrupted features: 3\n",
      "Saving >>> success\n",
      "========================================\n",
      "Loading validate dataset\n",
      "\t\t>> sample count: 759664\n",
      "\t> Transforming the validate dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gfalouji/miniconda3/envs/captn-nba/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:702: FutureWarning: The output of the 'no_scaler' transformer for column 'abs_ccs' has dtype Float64 and uses pandas.NA to represent null values. Storing this output in a numpy array can cause errors in downstream scikit-learn estimators, and inefficiencies. Starting with scikit-learn version 1.6, this will raise a ValueError. To avoid this problem you can (i) store the output in a pandas DataFrame by using ColumnTransformer.set_output(transform='pandas') or (ii) modify the input data or the 'no_scaler' transformer to avoid the presence of pandas.NA (for example by using pandas.DataFrame.astype).\n",
      "  warnings.warn(\n",
      "/home/gfalouji/miniconda3/envs/captn-nba/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:702: FutureWarning: The output of the 'no_scaler' transformer for column 'curv' has dtype Float64 and uses pandas.NA to represent null values. Storing this output in a numpy array can cause errors in downstream scikit-learn estimators, and inefficiencies. Starting with scikit-learn version 1.6, this will raise a ValueError. To avoid this problem you can (i) store the output in a pandas DataFrame by using ColumnTransformer.set_output(transform='pandas') or (ii) modify the input data or the 'no_scaler' transformer to avoid the presence of pandas.NA (for example by using pandas.DataFrame.astype).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t>> transformation complete\n",
      "\t> Dropping corrupted features\n",
      "\t\t>> number of dropped corrupted features: 3\n",
      "Saving >>> success\n",
      "========================================\n",
      "Loading test dataset\n",
      "\t\t>> sample count: 952965\n",
      "\t> Transforming the test dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gfalouji/miniconda3/envs/captn-nba/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:702: FutureWarning: The output of the 'no_scaler' transformer for column 'abs_ccs' has dtype Float64 and uses pandas.NA to represent null values. Storing this output in a numpy array can cause errors in downstream scikit-learn estimators, and inefficiencies. Starting with scikit-learn version 1.6, this will raise a ValueError. To avoid this problem you can (i) store the output in a pandas DataFrame by using ColumnTransformer.set_output(transform='pandas') or (ii) modify the input data or the 'no_scaler' transformer to avoid the presence of pandas.NA (for example by using pandas.DataFrame.astype).\n",
      "  warnings.warn(\n",
      "/home/gfalouji/miniconda3/envs/captn-nba/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:702: FutureWarning: The output of the 'no_scaler' transformer for column 'curv' has dtype Float64 and uses pandas.NA to represent null values. Storing this output in a numpy array can cause errors in downstream scikit-learn estimators, and inefficiencies. Starting with scikit-learn version 1.6, this will raise a ValueError. To avoid this problem you can (i) store the output in a pandas DataFrame by using ColumnTransformer.set_output(transform='pandas') or (ii) modify the input data or the 'no_scaler' transformer to avoid the presence of pandas.NA (for example by using pandas.DataFrame.astype).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t>> transformation complete\n",
      "\t> Dropping corrupted features\n",
      "\t\t>> number of dropped corrupted features: 3\n",
      "Saving >>> success\n",
      "========================================\n",
      "CPU times: user 28.8 s, sys: 11.4 s, total: 40.1 s\n",
      "Wall time: 40.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if SCALE:    \n",
    "    for k, v in import_paths.items():\n",
    "        # Load the dataset >\n",
    "        asset_df = pd.DataFrame()\n",
    "        print(f\"Loading {k} dataset\")\n",
    "        asset_df = load_df_to_dataset(v).data\n",
    "        \n",
    "        if DOWN_SAMPLE:  # Reset the index to move datetime index to a column and create a new numerical index >\n",
    "            asset_df.reset_index(inplace=True)  # Reset index to move datetime index to a column and create a new numerical index\n",
    "        \n",
    "        original_sample_cnt = asset_df.shape[0]\n",
    "        print(f'\\t\\t>> sample count: {original_sample_cnt}')\n",
    "        \n",
    "        # Transform the dataset >\n",
    "        print(f'\\t> Transforming the {k} dataset')\n",
    "        asset_df = pd.DataFrame(scale_transformer.transform(asset_df), columns=columns_in_order)\n",
    "        print(f'\\t\\t>> transformation complete')\n",
    "        \n",
    "        if DROP_CORRUPTED_FEATURES:  # Drop the corrupted features >\n",
    "            print(f\"\\t> Dropping corrupted features\")\n",
    "            asset_df = asset_df.drop(corrupted_features, axis=1)\n",
    "            print(f'\\t\\t>> number of dropped corrupted features: {len(corrupted_features)}')\n",
    "        \n",
    "        # Save the scaled df as parquet >\n",
    "        print('Saving >>>', end=' ')\n",
    "        if DOWN_SAMPLE:\n",
    "            asset_df.to_parquet(tvt_scaled_dir / f'scaled_cleaned_downsampled_extended_{k}_df.parquet')\n",
    "        else:\n",
    "            asset_df.to_parquet(tvt_scaled_dir / f'scaled_cleaned_extended_{k}_df.parquet')\n",
    "        print('success')\n",
    "        \n",
    "        del asset_df\n",
    "        print('=='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all good!\n"
     ]
    }
   ],
   "source": [
    "if DOWN_SAMPLE:\n",
    "    if load_df_to_dataset(import_paths['train']).data.shape[0] == load_df_to_dataset(tvt_scaled_dir / 'scaled_cleaned_downsampled_extended_train_df.parquet').data.shape[0]:\n",
    "        print(\"all good!\")\n",
    "else:\n",
    "    if load_df_to_dataset(import_paths['train']).data.shape[0] == load_df_to_dataset(tvt_scaled_dir / 'scaled_cleaned_extended_train_df.parquet').data.shape[0]:\n",
    "        print(\"all good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>datetime</th>\n",
       "      <th>obj_id</th>\n",
       "      <th>traj_id</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>season</th>\n",
       "      <th>part_of_day</th>\n",
       "      <th>...</th>\n",
       "      <th>rot_c</th>\n",
       "      <th>distance_c</th>\n",
       "      <th>dist_ww</th>\n",
       "      <th>dist_ra</th>\n",
       "      <th>dist_cl</th>\n",
       "      <th>dist_ma</th>\n",
       "      <th>speed_c</th>\n",
       "      <th>acc_c</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1648127580</td>\n",
       "      <td>2022-03-24 13:13:00</td>\n",
       "      <td>209221000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005813</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.006155</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>-0.548516</td>\n",
       "      <td>0.012355</td>\n",
       "      <td>0.170229</td>\n",
       "      <td>0.069312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1648127640</td>\n",
       "      <td>2022-03-24 13:14:00</td>\n",
       "      <td>209221000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.086218e-04</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.005801</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>0.006140</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>-0.572914</td>\n",
       "      <td>-0.115195</td>\n",
       "      <td>0.170248</td>\n",
       "      <td>0.069310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1648127700</td>\n",
       "      <td>2022-03-24 13:15:00</td>\n",
       "      <td>209221000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.674825e-04</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.005793</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>0.006131</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>-0.664865</td>\n",
       "      <td>-0.318543</td>\n",
       "      <td>0.170259</td>\n",
       "      <td>0.069308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1648127760</td>\n",
       "      <td>2022-03-24 13:16:00</td>\n",
       "      <td>209221000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.029244e-02</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.005791</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>0.006129</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>-0.572722</td>\n",
       "      <td>3.605529</td>\n",
       "      <td>0.170262</td>\n",
       "      <td>0.069308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1648127820</td>\n",
       "      <td>2022-03-24 13:17:00</td>\n",
       "      <td>209221000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002778e-10</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.005790</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>0.006128</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>-0.721387</td>\n",
       "      <td>-0.052948</td>\n",
       "      <td>0.170263</td>\n",
       "      <td>0.069309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        epoch            datetime       obj_id  traj_id  month_sin  \\\n",
       "0  1648127580 2022-03-24 13:13:00  209221000.0        0        1.0   \n",
       "1  1648127640 2022-03-24 13:14:00  209221000.0        0        1.0   \n",
       "2  1648127700 2022-03-24 13:15:00  209221000.0        0        1.0   \n",
       "3  1648127760 2022-03-24 13:16:00  209221000.0        0        1.0   \n",
       "4  1648127820 2022-03-24 13:17:00  209221000.0        0        1.0   \n",
       "\n",
       "      month_cos  hour_sin  hour_cos  season  part_of_day  ...         rot_c  \\\n",
       "0  6.123234e-17 -0.258819 -0.965926     0.0          1.0  ...  0.000000e+00   \n",
       "1  6.123234e-17 -0.258819 -0.965926     0.0          1.0  ...  4.086218e-04   \n",
       "2  6.123234e-17 -0.258819 -0.965926     0.0          1.0  ... -7.674825e-04   \n",
       "3  6.123234e-17 -0.258819 -0.965926     0.0          1.0  ...  4.029244e-02   \n",
       "4  6.123234e-17 -0.258819 -0.965926     0.0          1.0  ...  1.002778e-10   \n",
       "\n",
       "   distance_c   dist_ww   dist_ra   dist_cl   dist_ma   speed_c     acc_c  \\\n",
       "0    0.000000  0.005813  0.002512  0.006155  0.002512 -0.548516  0.012355   \n",
       "1    0.000383  0.005801  0.002498  0.006140  0.002498 -0.572914 -0.115195   \n",
       "2    0.000152  0.005793  0.002489  0.006131  0.002489 -0.664865 -0.318543   \n",
       "3    0.000383  0.005791  0.002487  0.006129  0.002487 -0.572722  3.605529   \n",
       "4    0.000010  0.005790  0.002486  0.006128  0.002486 -0.721387 -0.052948   \n",
       "\n",
       "        lon       lat  \n",
       "0  0.170229  0.069312  \n",
       "1  0.170248  0.069310  \n",
       "2  0.170259  0.069308  \n",
       "3  0.170262  0.069308  \n",
       "4  0.170263  0.069309  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if DOWN_SAMPLE:\n",
    "    display(load_df_to_dataset(tvt_scaled_dir / 'scaled_cleaned_downsampled_extended_train_df.parquet').data.head())\n",
    "else:\n",
    "    display(load_df_to_dataset(tvt_scaled_dir / 'scaled_cleaned_extended_train_df.parquet').data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captn-nba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
