{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Training](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "1. [Import dependencies](#toc1_)    \n",
    "2. [Load scaled parquet into Pandas DataFrame](#toc2_)\n",
    "3. [Dataloader](#toc3_)\n",
    "4. [Training](#toc4_)\n",
    "5. [Plotting](#toc5_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a id='toc1'></a>[Import dependencies](#toc1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> import libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "root_dir = Path.cwd().resolve().parent\n",
    "if root_dir.exists():\n",
    "    sys.path.append(str(root_dir))\n",
    "else:\n",
    "    raise FileNotFoundError('Root directory not found')\n",
    "\n",
    "#> import custom libraries\n",
    "from src.load import load_df_to_dataset\n",
    "from src.models import TransformerDenoiseAutoEncoder\n",
    "from src.train import train_and_evaluate\n",
    "from src.traj_dataloader import (TrajectoryDataset, \n",
    "                                 DenoiseAutoencoderSequencedDataset\n",
    "                                 )\n",
    "from src.plot import  plot_losses\n",
    "\n",
    "\n",
    "#> torch libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "#> Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scienceplots  # https://github.com/garrettj403/SciencePlots?tab=readme-ov-file\n",
    "plt.style.use(['science', 'grid', 'notebook'])  # , 'ieee'\n",
    "\n",
    "\n",
    "# %matplotlib inline\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_dir = root_dir.parent / 'data' / 'local' / 'aistraj' / 'tvt_assets'\n",
    "assets_dir = assets_dir.resolve()\n",
    "if not assets_dir.exists():\n",
    "    raise FileNotFoundError('Assets directory not found')\n",
    "\n",
    "models_dir = root_dir / 'models' / 'sga'\n",
    "models_dir = models_dir.resolve()\n",
    "if not models_dir.exists():\n",
    "    raise FileNotFoundError('Models directory not found')\n",
    "\n",
    "scaled_tvt_data_import_assets_dir = assets_dir / 'scaled' \n",
    "scaled_tvt_data_import_assets_dir = scaled_tvt_data_import_assets_dir.resolve()\n",
    "if not scaled_tvt_data_import_assets_dir.exists():\n",
    "    raise FileNotFoundError('Train-Validate-Test Pickled Data directory not found')\n",
    "\n",
    "extend_tvt_data_import_assets_dir = assets_dir / 'extended' \n",
    "extend_tvt_data_import_assets_dir = extend_tvt_data_import_assets_dir.resolve()\n",
    "if not extend_tvt_data_import_assets_dir.exists():\n",
    "    raise FileNotFoundError('Train-Validate-Test Pickled Data directory not found')\n",
    "\n",
    "tvt_data_import_assets_dir = assets_dir / 'original' \n",
    "tvt_data_import_assets_dir = tvt_data_import_assets_dir.resolve()\n",
    "if not tvt_data_import_assets_dir.exists():\n",
    "    raise FileNotFoundError('Train-Validate-Test Pickled Data directory not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a id='toc2_'></a>[Load scaled parquet into Pandas DataFrame](#toc2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to the pickle files\n",
    "train_pickle_path = scaled_tvt_data_import_assets_dir / 'scaled_cleaned_extended_train_df.parquet'\n",
    "validate_pickle_path = scaled_tvt_data_import_assets_dir / 'scaled_cleaned_extended_validate_df.parquet'\n",
    "test_pickle_path = scaled_tvt_data_import_assets_dir / 'scaled_cleaned_extended_test_df.parquet'\n",
    "\n",
    "train_df = load_df_to_dataset(train_pickle_path).data\n",
    "validate_df = load_df_to_dataset(validate_pickle_path).data\n",
    "test_df = load_df_to_dataset(test_pickle_path).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['epoch', 'datetime', 'obj_id', 'traj_id', 'month_sin', 'month_cos',\n",
       "       'hour_sin', 'hour_cos', 'season', 'part_of_day', 'aad', 'cdd',\n",
       "       'dir_ccs', 'cog_c', 'rot_c', 'distance_c', 'dist_ww', 'dist_ra',\n",
       "       'dist_cl', 'dist_ma', 'speed_c', 'acc_c', 'lon', 'lat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a id='toc3_'></a>[Dataloader](#toc3_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_columns: Index(['month_sin', 'month_cos', 'hour_sin', 'hour_cos', 'season',\n",
      "       'part_of_day', 'aad', 'cdd', 'dir_ccs', 'cog_c', 'rot_c', 'distance_c',\n",
      "       'dist_ww', 'dist_ra', 'dist_cl', 'dist_ma', 'speed_c', 'acc_c', 'lon',\n",
      "       'lat'],\n",
      "      dtype='object')\n",
      "n_features: 20\n",
      "row of train_dataset: 57444\n",
      "row of train_df: 14705500\n",
      "padding need for train_df: 164\n",
      "----------------------------------------\n",
      "feature_columns: Index(['month_sin', 'month_cos', 'hour_sin', 'hour_cos', 'season',\n",
      "       'part_of_day', 'aad', 'cdd', 'dir_ccs', 'cog_c', 'rot_c', 'distance_c',\n",
      "       'dist_ww', 'dist_ra', 'dist_cl', 'dist_ma', 'speed_c', 'acc_c', 'lon',\n",
      "       'lat'],\n",
      "      dtype='object')\n",
      "n_features: 20\n",
      "row of val_dataset: 14700\n",
      "row of val_df: 3763005\n",
      "padding need for val_df: 195\n"
     ]
    }
   ],
   "source": [
    "drop_features_list = ['epoch', 'datetime', 'obj_id', 'traj_id']\n",
    "\n",
    "train_dataset_seq = DenoiseAutoencoderSequencedDataset(train_df, drop_features_list, seq_len=256)\n",
    "val_dataset_seq = DenoiseAutoencoderSequencedDataset(validate_df, drop_features_list, seq_len=256)\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "train_dataloader_seq = DataLoader(train_dataset_seq, batch_size=batch_size, num_workers=16, shuffle=False, pin_memory=True)\n",
    "val_dataloader_seq = DataLoader(val_dataset_seq, batch_size=batch_size, num_workers=16, shuffle=False, pin_memory=True)\n",
    "\n",
    "print (f'feature_columns: {train_dataset_seq.feature_columns}')\n",
    "print (f'n_features: {train_dataset_seq.n_features}')\n",
    "print (f'row of train_dataset: {train_dataset_seq.total_sequences}')\n",
    "print (f'row of train_df: {train_dataset_seq.l_dataset}')\n",
    "print (f'padding need for train_df: {train_dataset_seq.padding_needed}')\n",
    "print (f'-'*40)\n",
    "print (f'feature_columns: {val_dataset_seq.feature_columns}')\n",
    "print (f'n_features: {val_dataset_seq.n_features}')\n",
    "print (f'row of val_dataset: {val_dataset_seq.total_sequences}')\n",
    "print (f'row of val_df: {val_dataset_seq.l_dataset}')\n",
    "print (f'padding need for val_df: {val_dataset_seq.padding_needed}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a id='toc4_'></a>[Training](#toc4_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = val_dataset_seq.n_features  \n",
    "d_model = 4  # Transformer model dimensions\n",
    "nhead = 4  # Number of heads of multi-attention mechanisms\n",
    "num_encoder_layers = 1  # Number of encoder layers\n",
    "num_decoder_layers = 1  # Number of decoder layers\n",
    "dim_feedforward = 256  # feedforward network dimension\n",
    "max_seq_length = 256  \n",
    "dropout_rate = 0.1\n",
    "\n",
    "model_transformerDAE = TransformerDenoiseAutoEncoder(input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, dropout_rate)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model_transformerDAE = model_transformerDAE.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model_transformerDAE.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Start training\n",
    "train_losses_transformerDAE, eval_losses_transformerDAE, encoded_features_transformerDAE, all_inputs_transformerDAE, all_reconstructions_transformerDAE = train_and_evaluate(model=model_transformerDAE, train_dataloader=train_dataloader_seq, eval_dataloader=val_dataloader_seq, test_dataloader=None, optimizer=optimizer, criterion=criterion, model_save_path=models_dir, pickle_save_path=models_dir, model_name = 'transformer_denoiseautoencoder_model_parquet_', epochs=100, mode='train', patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a id='toc5_'></a>[Plotting](#toc5_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses (train_losses_transformerDAE, eval_losses_transformerDAE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captn-nba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
