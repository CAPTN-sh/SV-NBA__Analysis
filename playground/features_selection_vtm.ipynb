{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Features Selection](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code to select the most important features for the model. \n",
    "\n",
    "Here filter mothod is used to select the features. The features are selected using the `feature_importance_` attribute of the model. The features are then ranked and the top 10 features are selected. The selected features are then used to train the model.\n",
    "\n",
    "Filter-based feature selection methods for unsupervised data typically rely on statistical measures or heuristic approaches to rank features based on their intrinsic characteristics rather than on a specific learning algorithm. Here are a few filter-based methods along with how you might associate each selected feature with its importance:\n",
    "\n",
    "+ Variance Threshold</br>\n",
    "Compute the variance of each feature. Features with low variance are less informative and can be removed.\n",
    "Associate the variance value directly as the feature importance.\n",
    "Correlation Coefficient:\n",
    "\n",
    "+ Calculate the correlation coefficient between each pair of features.</br>\n",
    "  Features highly correlated with other features might contain redundant information. You can select one of each highly correlated pair or remove one randomly.\n",
    "Associate the absolute value of the correlation coefficient as the feature importance.\n",
    "Mutual Information:\n",
    "\n",
    "+ Measure the mutual information between each feature and the cluster labels.</br>\n",
    "Features with high mutual information are more informative for clustering.\n",
    "Associate the mutual information value as the feature importance.\n",
    "Distance-based Methods:\n",
    "\n",
    "+ Compute the distance between instances in the feature space and analyze the distribution of distances.</br>\n",
    "Features that contribute to larger distances between instances might be more important for clustering.\n",
    "Associate the distance measure (e.g., mean distance or median distance) as the feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ``sklearn.feature_selection`` module is used for feature selection/dimensionality reduction.\n",
    "+ Goal:\n",
    "  + Improve estimators accuracy scores\n",
    "  + Avoiding overfitting\n",
    "  + Reduce the computational cost\n",
    "  + Improve the comprehensibility of the model\n",
    "+ There are three main strategies:\n",
    "  + Univariate statistics: Select the best features based on univariate statistical tests\n",
    "  + Model-based selection: Use a supervised model to judge the importance of each feature\n",
    "  + Iterative selection: Build a model on initial features and then iteratively remove the least important feature\n",
    "+ Feature selection methods can also be categorised into:\n",
    "  + Filter methods: Select features based on their scores in various statistical tests\n",
    "  + Wrapper methods: Select features based on the performance of a model trained with the selected features\n",
    "  + Embedded methods: Select features based on the importance of their contribution to the model\n",
    "+ Feature selection can be done in four ways:\n",
    "  + **SelectKBest**: Select features according to the k highest scores\n",
    "  + **SelectPercentile**: Select features according to a percentile of the highest scores\n",
    "  + **SelectFpr**: Select features based on a false positive rate test\n",
    "  + **SelectFdr**: Select features based on an estimated false discovery rate\n",
    "  + **SelectFwe**: Select features based on family-wise error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "1. [Features Selection](#toc1_)    \n",
    "1.1. [Dependencies and paths](#toc1_1_)    \n",
    "1.2. [Load the data](#toc1_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=true\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. <a id='toc1_1_'></a>[Dependencies and paths](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaf/miniconda3/envs/captn-nba/lib/python3.12/site-packages/skfeature/utility/construct_W.py:7: SyntaxWarning: invalid escape sequence '\\i'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "## DEPENDENCIES >>>\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Any, Optional, Callable, Union\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "from functools import partial\n",
    "\n",
    "# Add root directory to path for imports >\n",
    "root_dir = Path.cwd().resolve().parent\n",
    "if root_dir.exists():\n",
    "    sys.path.append(str(root_dir))\n",
    "else:\n",
    "    raise FileNotFoundError('Root directory not found')\n",
    "\n",
    "# import custom libraries >\n",
    "from src.load import load_multiple_trajectoryCollection_parallel_pickle as lmtp\n",
    "from src.load import load_datasets, load_df_to_dataset\n",
    "from src.traj_dataloader import (TrajectoryDataset, \n",
    "                                 create_dataloader, \n",
    "                                 separate_files_by_season, \n",
    "                                 split_data, \n",
    "                                 get_files,\n",
    "                                 AISDataset,\n",
    "                                 )\n",
    "from src.scaler import CustomMinMaxScaler, reduce_resolution\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import dotsi\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# torch libraries >\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.io import read_image\n",
    "\n",
    "# sklearn libraries >\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import (train_test_split, \n",
    "                                     GridSearchCV, \n",
    "                                     RandomizedSearchCV)#, HalvingGridSearchCV, HalvingRandomSearchCV)\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score \n",
    "# from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "# Features selection >\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import ( mutual_info_classif,\n",
    "                                       SelectKBest,\n",
    "                                       chi2,\n",
    "                                       VarianceThreshold,\n",
    "                                       RFE,\n",
    "                                       )\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector\n",
    "from skfeature.function.similarity_based import fisher_score\n",
    "\n",
    "# Hyperopt >\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Plot >\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scienceplots  # https://github.com/garrettj403/SciencePlots?tab=readme-ov-file\n",
    "plt.style.use(['science', 'grid', 'notebook'])  # , 'ieee'\n",
    "\n",
    "# Multiprocessing >\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "# Toy datasets >\n",
    "from sklearn.datasets import load_iris  # Sample dataset\n",
    "\n",
    "# %matplotlib inline\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs available: 64\n",
      "Number of CPUs to use: 57\n"
     ]
    }
   ],
   "source": [
    "## FLAGS & GLOBAL VALUES >>>\n",
    "\n",
    "# Down sample the resolution\n",
    "DOWN_SAMPLE = False  # used with SCALE and SAVE_SCALE to save the scaled data: (if True) with down sampled resolution, or with (not False) not.\n",
    "\n",
    "# Explore\n",
    "EXPLORE = True\n",
    "\n",
    "# Debug\n",
    "DEBUG = True\n",
    "\n",
    "# Develop\n",
    "DEVELOP = True\n",
    "\n",
    "# HYPERPARAMETER OPTIMISATION\n",
    "HYPEROPT = True\n",
    "\n",
    "if HYPEROPT:\n",
    "    OPTUNA = False # Optimise using Optuna\n",
    "    GRIDSEARCH = True  # Optimise using GridSearchCV\n",
    "    RANDOMSEARCH = False  # Optimise using RandomizedSearchCV\n",
    "\n",
    "# SAVE SELECTED FEATURES in root / models / selected_features\n",
    "SAVE_SELECT_FEATURES = True\n",
    "\n",
    "# WORKING SERVER\n",
    "AVAILABLE_SERVERS = ['ZS', 'PLOEN', 'KIEL', 'WYK']\n",
    "CURRENT_SERVER = AVAILABLE_SERVERS[0]\n",
    "\n",
    "# seed\n",
    "split_seed = 42\n",
    "\n",
    "# If DOWN_SAMPLE, define the target time resolution\n",
    "targeted_resolution_min = 1  # minute\n",
    "\n",
    "# TODO: The following featues are corrupted by containing NaNs. Fix this. For now, these columns are dropped\n",
    "corrupted_features = [\"stopped\", \"abs_ccs\", \"curv\"]\n",
    "\n",
    "\n",
    "# Use up to 70% of the available cpu cores\n",
    "n_jobs = joblib.cpu_count()\n",
    "print(\"Number of CPUs available:\", n_jobs)\n",
    "if CURRENT_SERVER == 'ZS':\n",
    "    n_jobs = int(0.9 * n_jobs)\n",
    "else:\n",
    "    n_jobs = int(0.7 * n_jobs)\n",
    "print(\"Number of CPUs to use:\", n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PATHS >>>\n",
    "# data dir\n",
    "data_dir = root_dir / 'data'\n",
    "data_dir = data_dir.resolve()\n",
    "if not data_dir.exists():\n",
    "    raise FileNotFoundError('Data directory not found')\n",
    "\n",
    "if CURRENT_SERVER == 'ZS':\n",
    "    # assets dir  # TODO: Used temporarly during the features seletion process. Remove this!\n",
    "    assets_dir = data_dir / 'assets'\n",
    "    assets_dir = assets_dir.resolve()\n",
    "    if not assets_dir.exists():\n",
    "        raise FileNotFoundError(f'Assets directory in {CURRENT_SERVER} not found')\n",
    "else:\n",
    "    # aistraj dir\n",
    "    assets_dir = data_dir / 'local' / 'aistraj'\n",
    "    assets_dir = assets_dir.resolve()\n",
    "    if not assets_dir.exists():\n",
    "        raise FileNotFoundError('Assets directory not found')\n",
    "\n",
    "    # train-validate-test (tvt) dir\n",
    "    tvt_assets_dir = assets_dir / 'tvt_assets'\n",
    "    tvt_assets_dir = tvt_assets_dir.resolve()\n",
    "    if not tvt_assets_dir.exists():\n",
    "        raise FileNotFoundError('Train-Validate-Test Assets directory not found')\n",
    "\n",
    "    # tvt: extended pickle dir\n",
    "    tvt_extended_dir = tvt_assets_dir / 'extended'\n",
    "    tvt_extended_dir = tvt_extended_dir.resolve()\n",
    "    if not tvt_extended_dir.exists():\n",
    "        raise FileNotFoundError('TVT Extended Pickled Data directory not found')\n",
    "\n",
    "    # tvt: scaled pickle dir\n",
    "    tvt_scaled_dir = tvt_assets_dir / 'scaled'\n",
    "    tvt_scaled_dir = tvt_scaled_dir.resolve()\n",
    "    if not tvt_scaled_dir.exists():\n",
    "        raise FileNotFoundError('TVT Scaled Pickled Data directory not found')\n",
    "\n",
    "    # tvt: logs dir\n",
    "    tvt_logs_dir = tvt_assets_dir / 'logs'\n",
    "    tvt_logs_dir = tvt_logs_dir.resolve()\n",
    "    if not tvt_logs_dir.exists():\n",
    "        raise FileNotFoundError('TVT logs directory not found')\n",
    "  \n",
    "  \n",
    "# models dir\n",
    "models_dir = root_dir / 'models'\n",
    "models_dir = models_dir.resolve()\n",
    "if not models_dir.exists():\n",
    "    raise FileNotFoundError('Models directory not found')    \n",
    "\n",
    "# Selected Features dir\n",
    "selected_features_dir = models_dir / 'selected_features'\n",
    "selected_features_dir = selected_features_dir.resolve()\n",
    "if not selected_features_dir.exists():\n",
    "    raise FileNotFoundError('selected features directory not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. <a id='toc1_2_'></a>[Load the data](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Select the paths of the scaled datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_paths = {'train': None, 'validate': None, 'test': None}\n",
    "\n",
    "if DOWN_SAMPLE:\n",
    "    import_paths = {\n",
    "                    'train': tvt_scaled_dir / 'scaled_cleaned_downsampled_extended_train_df.parquet',\n",
    "                    'validate': tvt_scaled_dir / 'scaled_cleaned_downsampled_extended_validate_df.parquet',\n",
    "                    'test': tvt_scaled_dir / 'scaled_cleaned_downsampled_extended_test_df.parquet'\n",
    "                    }\n",
    "else:  \n",
    "    if CURRENT_SERVER != 'ZS':\n",
    "        import_paths = {\n",
    "                        'train': tvt_scaled_dir / 'scaled_cleaned_extended_train_df.parquet',\n",
    "                        'validate': tvt_scaled_dir / 'scaled_cleaned_extended_validate_df.parquet',\n",
    "                        'test': tvt_scaled_dir / 'scaled_cleaned_extended_test_df.parquet'\n",
    "                        }\n",
    "    else:\n",
    "        import_paths = {\n",
    "                        'train': assets_dir / 'scaled_cleaned_extended_train_df.parquet',\n",
    "                        'validate': assets_dir / 'scaled_cleaned_extended_validate_df.parquet',\n",
    "                        'test': assets_dir / 'scaled_cleaned_extended_test_df.parquet'\n",
    "                        }\n",
    "        \n",
    "# Assets container >\n",
    "train_df, validate_df, test_df = None, None, None\n",
    "assets = {'train': train_df, 'validate': validate_df, 'test': test_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if not DEVELOP:  # Data is huge! don't use for exploring and developping\n",
    "#     train_df = load_df_to_dataset(data_path=import_paths['train'], use_dask=False).data  # Load the train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the validate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.75 s, sys: 7.07 s, total: 10.8 s\n",
      "Wall time: 1.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "validate_df = load_df_to_dataset(import_paths['validate'], use_dask=False).data  # Load the validate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. Cols: 24: Index(['epoch', 'datetime', 'obj_id', 'traj_id', 'month_sin', 'month_cos',\n",
      "       'hour_sin', 'hour_cos', 'season', 'part_of_day', 'aad', 'cdd',\n",
      "       'dir_ccs', 'cog_c', 'rot_c', 'distance_c', 'dist_ww', 'dist_ra',\n",
      "       'dist_cl', 'dist_ma', 'speed_c', 'acc_c', 'lon', 'lat'],\n",
      "      dtype='object')\n",
      "\n",
      "Num. Samples: 14705500\n"
     ]
    }
   ],
   "source": [
    "if EXPLORE:\n",
    "    columns = validate_df.columns\n",
    "    print(f\"Num. Cols: {len(columns)}: {columns}\")\n",
    "    print()\n",
    "    print(f\"Num. Samples: {validate_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>datetime</th>\n",
       "      <th>obj_id</th>\n",
       "      <th>traj_id</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>season</th>\n",
       "      <th>part_of_day</th>\n",
       "      <th>...</th>\n",
       "      <th>rot_c</th>\n",
       "      <th>distance_c</th>\n",
       "      <th>dist_ww</th>\n",
       "      <th>dist_ra</th>\n",
       "      <th>dist_cl</th>\n",
       "      <th>dist_ma</th>\n",
       "      <th>speed_c</th>\n",
       "      <th>acc_c</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.470550e+07</td>\n",
       "      <td>14705500</td>\n",
       "      <td>1.470550e+07</td>\n",
       "      <td>1.470550e+07</td>\n",
       "      <td>1.470550e+07</td>\n",
       "      <td>1.470550e+07</td>\n",
       "      <td>1.470550e+07</td>\n",
       "      <td>1.470550e+07</td>\n",
       "      <td>1.470550e+07</td>\n",
       "      <td>1.470550e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.470550e+07</td>\n",
       "      <td>1.470550e+07</td>\n",
       "      <td>1.470550e+07</td>\n",
       "      <td>1.470550e+07</td>\n",
       "      <td>1.470550e+07</td>\n",
       "      <td>1.470550e+07</td>\n",
       "      <td>1.470550e+07</td>\n",
       "      <td>1.470550e+07</td>\n",
       "      <td>1.470550e+07</td>\n",
       "      <td>1.470550e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.666974e+09</td>\n",
       "      <td>2022-10-28 16:13:28.685266944</td>\n",
       "      <td>2.375563e+08</td>\n",
       "      <td>8.652038e-01</td>\n",
       "      <td>6.713849e-02</td>\n",
       "      <td>-5.350965e-01</td>\n",
       "      <td>6.619752e-02</td>\n",
       "      <td>-3.610991e-01</td>\n",
       "      <td>7.613928e-01</td>\n",
       "      <td>7.803357e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.199664e-04</td>\n",
       "      <td>8.994076e-06</td>\n",
       "      <td>4.779191e-03</td>\n",
       "      <td>2.823630e-03</td>\n",
       "      <td>5.197064e-03</td>\n",
       "      <td>2.823630e-03</td>\n",
       "      <td>1.092504e-01</td>\n",
       "      <td>-5.415504e-01</td>\n",
       "      <td>1.716905e-01</td>\n",
       "      <td>6.982459e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.648080e+09</td>\n",
       "      <td>2022-03-24 00:00:00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.000000e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.090376e-03</td>\n",
       "      <td>1.545662e-03</td>\n",
       "      <td>3.682328e-03</td>\n",
       "      <td>1.545662e-03</td>\n",
       "      <td>-6.552631e-01</td>\n",
       "      <td>-2.205075e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.654656e+09</td>\n",
       "      <td>2022-06-08 02:32:17.500000</td>\n",
       "      <td>2.113416e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.949996e-04</td>\n",
       "      <td>1.798412e-06</td>\n",
       "      <td>3.843537e-03</td>\n",
       "      <td>2.221549e-03</td>\n",
       "      <td>4.462599e-03</td>\n",
       "      <td>2.221549e-03</td>\n",
       "      <td>-5.003267e-01</td>\n",
       "      <td>-5.611262e-01</td>\n",
       "      <td>1.707412e-01</td>\n",
       "      <td>6.928030e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.663483e+09</td>\n",
       "      <td>2022-09-18 06:32:10</td>\n",
       "      <td>2.118447e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.910451e-06</td>\n",
       "      <td>4.786457e-03</td>\n",
       "      <td>2.484184e-03</td>\n",
       "      <td>5.204943e-03</td>\n",
       "      <td>2.484184e-03</td>\n",
       "      <td>3.653112e-03</td>\n",
       "      <td>-3.837946e-04</td>\n",
       "      <td>1.715423e-01</td>\n",
       "      <td>6.953888e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.683541e+09</td>\n",
       "      <td>2023-05-08 10:24:40</td>\n",
       "      <td>2.453990e+08</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>7.071068e-01</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.929777e-04</td>\n",
       "      <td>1.374012e-05</td>\n",
       "      <td>5.713276e-03</td>\n",
       "      <td>3.388410e-03</td>\n",
       "      <td>5.995282e-03</td>\n",
       "      <td>3.388410e-03</td>\n",
       "      <td>4.871682e-01</td>\n",
       "      <td>4.392827e-01</td>\n",
       "      <td>1.725441e-01</td>\n",
       "      <td>7.059271e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.688170e+09</td>\n",
       "      <td>2023-07-01 00:00:00</td>\n",
       "      <td>1.000000e+09</td>\n",
       "      <td>3.200000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000e-02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.300795e+04</td>\n",
       "      <td>9.325662e+03</td>\n",
       "      <td>9.216453e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.360122e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.435024e+07</td>\n",
       "      <td>2.382782e+00</td>\n",
       "      <td>6.518146e-01</td>\n",
       "      <td>5.331998e-01</td>\n",
       "      <td>6.910082e-01</td>\n",
       "      <td>6.226822e-01</td>\n",
       "      <td>9.608422e-01</td>\n",
       "      <td>7.965457e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>9.468249e-03</td>\n",
       "      <td>3.591384e-04</td>\n",
       "      <td>1.086893e-03</td>\n",
       "      <td>9.401402e-04</td>\n",
       "      <td>9.237291e-04</td>\n",
       "      <td>9.401402e-04</td>\n",
       "      <td>4.212552e+01</td>\n",
       "      <td>7.913628e+02</td>\n",
       "      <td>1.133481e-03</td>\n",
       "      <td>1.142910e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              epoch                       datetime        obj_id  \\\n",
       "count  1.470550e+07                       14705500  1.470550e+07   \n",
       "mean   1.666974e+09  2022-10-28 16:13:28.685266944  2.375563e+08   \n",
       "min    1.648080e+09            2022-03-24 00:00:00  0.000000e+00   \n",
       "25%    1.654656e+09     2022-06-08 02:32:17.500000  2.113416e+08   \n",
       "50%    1.663483e+09            2022-09-18 06:32:10  2.118447e+08   \n",
       "75%    1.683541e+09            2023-05-08 10:24:40  2.453990e+08   \n",
       "max    1.688170e+09            2023-07-01 00:00:00  1.000000e+09   \n",
       "std    1.360122e+07                            NaN  6.435024e+07   \n",
       "\n",
       "            traj_id     month_sin     month_cos      hour_sin      hour_cos  \\\n",
       "count  1.470550e+07  1.470550e+07  1.470550e+07  1.470550e+07  1.470550e+07   \n",
       "mean   8.652038e-01  6.713849e-02 -5.350965e-01  6.619752e-02 -3.610991e-01   \n",
       "min    0.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00   \n",
       "25%    0.000000e+00 -5.000000e-01 -1.000000e+00 -5.000000e-01 -8.660254e-01   \n",
       "50%    0.000000e+00  1.224647e-16 -8.660254e-01  1.224647e-16 -5.000000e-01   \n",
       "75%    1.000000e+00  5.000000e-01 -1.836970e-16  7.071068e-01  6.123234e-17   \n",
       "max    3.200000e+01  1.000000e+00  8.660254e-01  1.000000e+00  1.000000e+00   \n",
       "std    2.382782e+00  6.518146e-01  5.331998e-01  6.910082e-01  6.226822e-01   \n",
       "\n",
       "             season   part_of_day  ...         rot_c    distance_c  \\\n",
       "count  1.470550e+07  1.470550e+07  ...  1.470550e+07  1.470550e+07   \n",
       "mean   7.613928e-01  7.803357e-01  ... -1.199664e-04  8.994076e-06   \n",
       "min    0.000000e+00  0.000000e+00  ... -5.000000e-02  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  ... -3.949996e-04  1.798412e-06   \n",
       "50%    0.000000e+00  1.000000e+00  ...  0.000000e+00  7.910451e-06   \n",
       "75%    1.000000e+00  1.000000e+00  ...  3.929777e-04  1.374012e-05   \n",
       "max    3.000000e+00  2.000000e+00  ...  5.000000e-02  1.000000e+00   \n",
       "std    9.608422e-01  7.965457e-01  ...  9.468249e-03  3.591384e-04   \n",
       "\n",
       "            dist_ww       dist_ra       dist_cl       dist_ma       speed_c  \\\n",
       "count  1.470550e+07  1.470550e+07  1.470550e+07  1.470550e+07  1.470550e+07   \n",
       "mean   4.779191e-03  2.823630e-03  5.197064e-03  2.823630e-03  1.092504e-01   \n",
       "min    3.090376e-03  1.545662e-03  3.682328e-03  1.545662e-03 -6.552631e-01   \n",
       "25%    3.843537e-03  2.221549e-03  4.462599e-03  2.221549e-03 -5.003267e-01   \n",
       "50%    4.786457e-03  2.484184e-03  5.204943e-03  2.484184e-03  3.653112e-03   \n",
       "75%    5.713276e-03  3.388410e-03  5.995282e-03  3.388410e-03  4.871682e-01   \n",
       "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  8.300795e+04   \n",
       "std    1.086893e-03  9.401402e-04  9.237291e-04  9.401402e-04  4.212552e+01   \n",
       "\n",
       "              acc_c           lon           lat  \n",
       "count  1.470550e+07  1.470550e+07  1.470550e+07  \n",
       "mean  -5.415504e-01  1.716905e-01  6.982459e-02  \n",
       "min   -2.205075e+06  0.000000e+00  0.000000e+00  \n",
       "25%   -5.611262e-01  1.707412e-01  6.928030e-02  \n",
       "50%   -3.837946e-04  1.715423e-01  6.953888e-02  \n",
       "75%    4.392827e-01  1.725441e-01  7.059271e-02  \n",
       "max    9.325662e+03  9.216453e-01  1.000000e+00  \n",
       "std    7.913628e+02  1.133481e-03  1.142910e-03  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if EXPLORE:\n",
    "    display(validate_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14705500 entries, 0 to 14705499\n",
      "Data columns (total 24 columns):\n",
      " #   Column       Dtype         \n",
      "---  ------       -----         \n",
      " 0   epoch        int64         \n",
      " 1   datetime     datetime64[ns]\n",
      " 2   obj_id       int64         \n",
      " 3   traj_id      int64         \n",
      " 4   month_sin    float64       \n",
      " 5   month_cos    float64       \n",
      " 6   hour_sin     float64       \n",
      " 7   hour_cos     float64       \n",
      " 8   season       int64         \n",
      " 9   part_of_day  int64         \n",
      " 10  aad          float64       \n",
      " 11  cdd          float64       \n",
      " 12  dir_ccs      float64       \n",
      " 13  cog_c        float64       \n",
      " 14  rot_c        float64       \n",
      " 15  distance_c   float64       \n",
      " 16  dist_ww      float64       \n",
      " 17  dist_ra      float64       \n",
      " 18  dist_cl      float64       \n",
      " 19  dist_ma      float64       \n",
      " 20  speed_c      float64       \n",
      " 21  acc_c        float64       \n",
      " 22  lon          float64       \n",
      " 23  lat          float64       \n",
      "dtypes: datetime64[ns](1), float64(18), int64(5)\n",
      "memory usage: 2.6 GB\n"
     ]
    }
   ],
   "source": [
    "if EXPLORE:\n",
    "    validate_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if not DEVELOP:  # Data is huge! don't use for exploring and developping\n",
    "#     test_df = load_df_to_dataset(import_paths['test'], use_dask=False).data  # Load the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Concat the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the datasets >\n",
    "asset_df = validate_df  # pd.concat([train_df, validate_df, test_df], axis=0)\n",
    "\n",
    "# # Sort the dataset by epoch >\n",
    "# asset_df = asset_df.sort_values(by='epoch', ascending=True)\n",
    "\n",
    "# # Reset the index >\n",
    "# asset_df = asset_df.reset_index(drop=True)\n",
    "\n",
    "# # Display the dataset's head >\n",
    "# if EXPLORE:\n",
    "#     asset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter-based features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cols not to study: ['epoch', 'datetime', 'obj_id', 'traj_id']\n"
     ]
    }
   ],
   "source": [
    "cols_not_to_study = ['epoch', 'datetime', 'obj_id', 'traj_id', 'stopped', 'curv']\n",
    "\n",
    "# Check that the column in cols_not_to_study are in the dataset, otherwise remove them from the list >\n",
    "cols_not_to_study = [col for col in cols_not_to_study if col in asset_df.columns]\n",
    "\n",
    "print(f\"Cols not to study: {cols_not_to_study}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Threshold Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The variance threshold method is a simple unsupervised feature selection method. It removes all features whose variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples.\n",
    "+ One of the main assumptions of this method is that features with a higher variance may contain more useful information. In practice, variance thresholding may not be very useful for regression tasks, but it can be useful for classification tasks, especially for binary classification and clustering tasks.\n",
    "+ The variance threshold method is a simple and effective method for feature selection. It is a good starting point for feature selection and is especially useful for removing noisy and irrelevant features.\n",
    "+ Feature variance can be used a measure of feature importance. Features with low variance are less informative and can be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Optuna objective function for the optimisation of ``threshold`` hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 384 µs, sys: 259 µs, total: 643 µs\n",
      "Wall time: 657 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a copy of the dataset and drop the columns not to study >\n",
    "df = asset_df.drop(columns=cols_not_to_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Find the best threshold value for the variance threshold method using Optuna. Using the silhouette score with k-means clustering.\n",
    "  > **NOTE**:</br> Assuming that the number of clusters is $10$.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_threshold_feature_selection(data: pd.DataFrame, threshold: float) -> Tuple[VarianceThreshold, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Perform feature selection using variance threshold.\n",
    "    Assign the feature_importance based on the normalised variance of the features. \n",
    "    The lower the variance, the less important the feature.\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame containing the features.\n",
    "        threshold (float): The threshold value for variance.\n",
    "\n",
    "    Returns:\n",
    "        Union[callable, pd.DataFrame]:\n",
    "            [callable]: is the fitted VarianceThreshold object.\n",
    "            [pd.DataFrame]: is the selected features in descending order.\n",
    "                            The DataFrame contains two columns:\n",
    "                                - `selected_features`: The selected features.\n",
    "                                - `feature_importance`: The corresponding feature importance values.\n",
    "    \"\"\"\n",
    "    # Instantiate a place holder for the variance threshold method (vtm) selected features >\n",
    "    fs_df = pd.DataFrame(columns=['selected_features', 'feature_importance'])\n",
    "\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "    selector.fit(data)\n",
    "    selected_features = data.columns[selector.get_support()]\n",
    "    feature_importance = selector.variances_\n",
    "    # L2 normalisation\n",
    "    feature_importance = Normalizer().fit_transform(feature_importance)\n",
    "    \n",
    "    # put the data in fs_df >\n",
    "    fs_df['selected_features'] = selected_features\n",
    "    fs_df['feature_importance'] = feature_importance\n",
    "    \n",
    "    return selector, fs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Define an objective function for Optuna >>\n",
    "if HYPEROPT:\n",
    "    if OPTUNA:  # Use Optuna for hyperparameter optimisation\n",
    "        def objective(trial, \n",
    "                      df: pd.DataFrame, \n",
    "                      cluster: Callable,\n",
    "                      n_clusters: int, \n",
    "                      random_state: Optional[int]=42, \n",
    "                      score_metric: Optional[Callable]=silhouette_score,\n",
    "                      steps: Optional[float]=0.1):\n",
    "            \"\"\"Optimization objective function for feature selection.\n",
    "\n",
    "            This function takes a trial object, a DataFrame, and optional parameters for the number of clusters and random state.\n",
    "            It performs feature selection using the VarianceThreshold method and trains a clustering model (e.g., KMeans) on the selected features.\n",
    "            The silhouette score is then calculated and returned as the optimization objective.\n",
    "\n",
    "            Args:\n",
    "                trial (optuna.Trial): The trial object used for optimization.\n",
    "                df (pd.DataFrame): The input DataFrame containing the features.\n",
    "                cluster (Callable): The clustering algorithm to be used.\n",
    "                n_clusters (int): The number of clusters for the clustering algorithm.\n",
    "                random_state (int, optional): The random state for reproducibility. Defaults to 42.\n",
    "                score_metric (Callable, optional): The scoring metric used to evaluate the clustering model. Defaults to sklearn.metrics.silhouette_score.\n",
    "                steps (float, optional): The step size for the threshold search space. Defaults to 0.2.\n",
    "\n",
    "            Returns:\n",
    "                float: The silhouette score of the clustering model trained on the selected features.\n",
    "            \"\"\"\n",
    "            # Print the current trial number\n",
    "            print(\"Running Trial Number:\", trial.number)\n",
    "            \n",
    "            # Define the search space for the threshold\n",
    "            threshold = trial.suggest_discrete_uniform(name='threshold', low=0, high=1, q=steps)  # Limit to 5 values between 0 and 1\n",
    "            \n",
    "            # Instantiate the VarianceThreshold object with the suggested threshold\n",
    "            selector, _ = variance_threshold_feature_selection(df, threshold)\n",
    "            \n",
    "            # Apply the selector to the data\n",
    "            x_selected = selector.transform(df)\n",
    "            \n",
    "            # Train a clustering model (e.g., KMeans) on the selected features\n",
    "            clusterer = cluster(n_clusters=n_clusters, random_state=random_state)\n",
    "            clusters = clusterer.fit_predict(x_selected)\n",
    "            \n",
    "            # Calculate silhouette score\n",
    "            silhouette = score_metric(x_selected, clusters)\n",
    "            return silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## TOY >>\n",
    "# X = df\n",
    "\n",
    "# # Define the threshold range\n",
    "# threshold_range = np.linspace(0, 0.5, 5)\n",
    "\n",
    "# # Define the parameter grid for RandomizedSearchCV\n",
    "# param_grid = {'vt__threshold': threshold_range}\n",
    "\n",
    "# # Initialize the pipeline with VarianceThreshold and KMeans clustering\n",
    "# pipeline = Pipeline([\n",
    "#     ('vt', VarianceThreshold()),\n",
    "#     ('kmeans', KMeans(n_clusters=30))\n",
    "# ])\n",
    "\n",
    "# # Define a function to compute silhouette score\n",
    "# def silhouette_scorer(estimator, X):\n",
    "#     labels = estimator.predict(X)\n",
    "#     return silhouette_score(X, labels)\n",
    "\n",
    "# # Initialize RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(estimator=pipeline,\n",
    "#                                    param_distributions=param_grid,\n",
    "#                                    scoring=silhouette_scorer,\n",
    "#                                    n_iter=20,  # Adjust the number of iterations as needed\n",
    "#                                    cv=5,       # Adjust cross-validation folds as needed\n",
    "#                                    random_state=42)\n",
    "\n",
    "# # Fit RandomizedSearchCV\n",
    "# random_search.fit(X)\n",
    "\n",
    "# # Print the best parameters and best score\n",
    "# print(\"Best threshold:\", random_search.best_params_)\n",
    "# print(\"Best silhouette score:\", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Hyperparameter Opt >\n",
    "best_threshold = None\n",
    "if HYPEROPT:\n",
    "        # Common parameters for the optimisation >\n",
    "        params = {'cluster': KMeans,\n",
    "                  'n_clusters': 30,\n",
    "                  'random_state': 42,\n",
    "                  'metric': silhouette_score,\n",
    "                  'n_iter': 100,\n",
    "                  'step': 0.2,\n",
    "                  'n_jobs': n_jobs}\n",
    "        if OPTUNA:\n",
    "                study_params = {'direction': 'maximize'}\n",
    "\n",
    "                # Create a study object and optimize the objective function >\n",
    "                study = optuna.create_study(direction=study_params['direction'])\n",
    "\n",
    "                # Use the validation set only for optimisation >\n",
    "                study.optimize(partial(objective,\n",
    "                                       df=df,\n",
    "                                       cluster=params['cluster'],\n",
    "                                       n_clusters=params['n_clusters'],\n",
    "                                       random_state=params['random_state'],\n",
    "                                       score_metric=params['metric'],\n",
    "                                       steps=params['step']), \n",
    "                        n_trials=params['n_iter'],\n",
    "                        n_jobs=params['n_jobs'])\n",
    "                # study.optimize(lambda trial: objective(trial, \n",
    "                #                                        df=df, \n",
    "                #                                        n_clusters=study_params.n_clusters, \n",
    "                #                                        random_state=split_seed), \n",
    "                #                n_trials=study_params.n_trials,\n",
    "                #                n_jobs=study_params.n_jobs)\n",
    "                # study.optimize(objective, n_trials=study_params.n_trials)\n",
    "\n",
    "                # Get the best threshold\n",
    "                best_threshold = study.best_params['threshold']\n",
    "                print(\"Best Threshold:\", best_threshold)\n",
    "\n",
    "                # Free up memory >\n",
    "                del study\n",
    "\n",
    "        if GRIDSEARCH:\n",
    "                # Define the parameter grid for RandomizedSearchCV\n",
    "                param_grid = {'vt__threshold': np.arange(0, 1, params['step'])}\n",
    "\n",
    "                # Initialize the pipeline with VarianceThreshold and KMeans clustering\n",
    "                clusterer = params['cluster']\n",
    "                pipeline = Pipeline([('vt', VarianceThreshold()),\n",
    "                                     ('kmeans', clusterer(n_clusters=params['n_clusters'], \n",
    "                                                          random_state=params['random_state']))\n",
    "                                     ])\n",
    "\n",
    "                # Define a function to compute silhouette score\n",
    "                def silhouette_scorer(estimator, X):\n",
    "                        labels = estimator.predict(X)\n",
    "                        return silhouette_score(X, labels)\n",
    "\n",
    "                # Initialize RandomizedSearchCV\n",
    "                grid_search = GridSearchCV(estimator=pipeline,\n",
    "                                           param_grid=param_grid,\n",
    "                                           scoring=silhouette_scorer,\n",
    "                                           cv=None,\n",
    "                                           n_jobs=params['n_jobs'],\n",
    "                                           verbose=1)\n",
    "\n",
    "                # Fit RandomizedSearchCV\n",
    "                grid_search.fit(df)\n",
    "\n",
    "                # Print the best parameters and best score\n",
    "                best_threshold = grid_search.best_params_['threshold']\n",
    "                \n",
    "                print(\"Best threshold:\", grid_search.best_params_['threshold'])\n",
    "                print(\"Best silhouette score:\", grid_search.best_score_)\n",
    "\n",
    "                # Free up memory >\n",
    "                del randomized_search\n",
    "                \n",
    "        if RANDOMSEARCH:\n",
    "                # Define the parameter grid for RandomizedSearchCV\n",
    "                param_grid = {'vt__threshold': np.arange(0, 1, params['step'])}\n",
    "\n",
    "                # Initialize the pipeline with VarianceThreshold and KMeans clustering\n",
    "                clusterer = params['cluster']\n",
    "                pipeline = Pipeline([('vt', VarianceThreshold()),\n",
    "                                     ('kmeans', clusterer(n_clusters=params['n_clusters'], \n",
    "                                                          random_state=params['random_state']))\n",
    "                                     ])\n",
    "\n",
    "                # Define a function to compute silhouette score\n",
    "                def silhouette_scorer(estimator, X):\n",
    "                        labels = estimator.predict(X)\n",
    "                        return silhouette_score(X, labels)\n",
    "\n",
    "                # Initialize RandomizedSearchCV\n",
    "                random_search = RandomizedSearchCV(estimator=pipeline,\n",
    "                                                   param_distributions=param_grid,\n",
    "                                                   scoring=silhouette_scorer,\n",
    "                                                   n_iter=params['n_iter'],\n",
    "                                                   cv=None,\n",
    "                                                   random_state=params['random_state'],\n",
    "                                                   n_jobs=params['n_jobs'],\n",
    "                                                   verbose=1)\n",
    "\n",
    "                # Fit RandomizedSearchCV\n",
    "                random_search.fit(df)\n",
    "\n",
    "                # Print the best parameters and best score\n",
    "                best_threshold = random_search.best_params_['threshold']\n",
    "                \n",
    "                print(\"Best threshold:\", random_search.best_params_['threshold'])\n",
    "                print(\"Best silhouette score:\", random_search.best_score_)\n",
    "\n",
    "                # Free up memory >\n",
    "                del randomized_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# If HYPEROPT, then use the optimised threshold, otherwise use the default threshold >\n",
    "threshold = None\n",
    "\n",
    "if HYPEROPT:\n",
    "    threshold = best_threshold\n",
    "else:\n",
    "    threshold = 0.1\n",
    "    \n",
    "# Selecte features and return scores >\n",
    "selector, fs_df = variance_threshold_feature_selection(df, threshold)\n",
    "# features_selected = selector.transform(df)\n",
    "\n",
    "display(fs_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# best_selector = VarianceThreshold(threshold=threshold)\n",
    "# X_selected = best_selector.fit_transform(df)\n",
    "\n",
    "# # Print out the selected features\n",
    "# selected_features = df.columns[best_selector.get_support(indices=True)]\n",
    "\n",
    "\n",
    "# # # Sort the selected features in alphabetical order\n",
    "# # selected_features = sorted(selected_features)\n",
    "\n",
    "# # Since the VTM does not provide a weight for each selected feature, we will create a uniform distribution of weights >\n",
    "# weight = 1 / len(selected_features)  # Calculate the weight for each selected feature\n",
    "# weights = [weight] * len(selected_features)  # Create a uniform distribution of weights\n",
    "\n",
    "# selected_features_vtm['selected_features'] = selected_features\n",
    "# selected_features_vtm['threshold'] = weights\n",
    "\n",
    "# print(\"Selected Features:\"), display(selected_features_vtm)\n",
    "\n",
    "# Free up memory\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the selected features to the models directory >\n",
    "if SAVE_SELECT_FEATURES:\n",
    "    fs_df.to_csv(selected_features_dir / 'new_selected_features_vtm.csv', index=False)\n",
    "    print(\"Selected Features saved to:\", selected_features_dir / 'new_selected_features_vtm.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captn-nba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
