{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Features Selection](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code to select the most important features for the model. \n",
    "\n",
    "Here filter mothod is used to select the features. The features are selected using the `feature_importance_` attribute of the model. The features are then ranked and the top 10 features are selected. The selected features are then used to train the model.\n",
    "\n",
    "Filter-based feature selection methods for unsupervised data typically rely on statistical measures or heuristic approaches to rank features based on their intrinsic characteristics rather than on a specific learning algorithm. Here are a few filter-based methods along with how you might associate each selected feature with its importance:\n",
    "\n",
    "+ Variance Threshold</br>\n",
    "Compute the variance of each feature. Features with low variance are less informative and can be removed.\n",
    "Associate the variance value directly as the feature importance.\n",
    "Correlation Coefficient:\n",
    "\n",
    "+ Calculate the correlation coefficient between each pair of features.</br>\n",
    "  Features highly correlated with other features might contain redundant information. You can select one of each highly correlated pair or remove one randomly.\n",
    "Associate the absolute value of the correlation coefficient as the feature importance.\n",
    "Mutual Information:\n",
    "\n",
    "+ Measure the mutual information between each feature and the cluster labels.</br>\n",
    "Features with high mutual information are more informative for clustering.\n",
    "Associate the mutual information value as the feature importance.\n",
    "Distance-based Methods:\n",
    "\n",
    "+ Compute the distance between instances in the feature space and analyze the distribution of distances.</br>\n",
    "Features that contribute to larger distances between instances might be more important for clustering.\n",
    "Associate the distance measure (e.g., mean distance or median distance) as the feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ``sklearn.feature_selection`` module is used for feature selection/dimensionality reduction.\n",
    "+ Goal:\n",
    "  + Improve estimators accuracy scores\n",
    "  + Avoiding overfitting\n",
    "  + Reduce the computational cost\n",
    "  + Improve the comprehensibility of the model\n",
    "+ There are three main strategies:\n",
    "  + Univariate statistics: Select the best features based on univariate statistical tests\n",
    "  + Model-based selection: Use a supervised model to judge the importance of each feature\n",
    "  + Iterative selection: Build a model on initial features and then iteratively remove the least important feature\n",
    "+ Feature selection methods can also be categorised into:\n",
    "  + Filter methods: Select features based on their scores in various statistical tests\n",
    "  + Wrapper methods: Select features based on the performance of a model trained with the selected features\n",
    "  + Embedded methods: Select features based on the importance of their contribution to the model\n",
    "+ Feature selection can be done in four ways:\n",
    "  + **SelectKBest**: Select features according to the k highest scores\n",
    "  + **SelectPercentile**: Select features according to a percentile of the highest scores\n",
    "  + **SelectFpr**: Select features based on a false positive rate test\n",
    "  + **SelectFdr**: Select features based on an estimated false discovery rate\n",
    "  + **SelectFwe**: Select features based on family-wise error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "1. [Features Selection](#toc1_)    \n",
    "1.1. [Dependencies and paths](#toc1_1_)    \n",
    "1.2. [Load the data](#toc1_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=true\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. <a id='toc1_1_'></a>[Dependencies and paths](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEPENDENCIES >>>\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Any, Optional, Callable, Union\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "from functools import partial\n",
    "\n",
    "# Add root directory to path for imports >\n",
    "root_dir = Path.cwd().resolve().parent\n",
    "if root_dir.exists():\n",
    "    sys.path.append(str(root_dir))\n",
    "else:\n",
    "    raise FileNotFoundError('Root directory not found')\n",
    "\n",
    "# import custom libraries >\n",
    "from src.load import load_multiple_trajectoryCollection_parallel_pickle as lmtp\n",
    "from src.load import load_datasets, load_df_to_dataset\n",
    "from src.traj_dataloader import (TrajectoryDataset, \n",
    "                                 create_dataloader, \n",
    "                                 separate_files_by_season, \n",
    "                                 split_data, \n",
    "                                 get_files,\n",
    "                                 AISDataset,\n",
    "                                 )\n",
    "from src.scaler import CustomMinMaxScaler, reduce_resolution\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import dotsi\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "# torch libraries >\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.io import read_image\n",
    "\n",
    "# sklearn libraries >\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import (train_test_split, \n",
    "                                     GridSearchCV, \n",
    "                                     RandomizedSearchCV)#, HalvingGridSearchCV, HalvingRandomSearchCV)\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score \n",
    "# from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "# Features selection >\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import ( mutual_info_classif,\n",
    "                                       SelectKBest,\n",
    "                                       chi2,\n",
    "                                       VarianceThreshold,\n",
    "                                       RFE,\n",
    "                                       )\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector\n",
    "from skfeature.function.similarity_based import fisher_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Hyperopt >\n",
    "import optuna\n",
    "\n",
    "# Mask warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Mask all warnings\n",
    "# from sklearn.exceptions import ConvergenceWarning\n",
    "# warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Plot >\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scienceplots  # https://github.com/garrettj403/SciencePlots?tab=readme-ov-file\n",
    "plt.style.use(['science', 'grid', 'notebook'])  # , 'ieee'\n",
    "\n",
    "# Multiprocessing >\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "# Toy datasets >\n",
    "from sklearn.datasets import load_iris  # Sample dataset\n",
    "\n",
    "# %matplotlib inline\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FLAGS & GLOBAL VALUES >>>\n",
    "\n",
    "# Down sample the resolution\n",
    "DOWN_SAMPLE = False  # used with SCALE and SAVE_SCALE to save the scaled data: (if True) with down sampled resolution, or with (not False) not.\n",
    "\n",
    "# Explore\n",
    "EXPLORE = True\n",
    "\n",
    "# Debug\n",
    "DEBUG = True\n",
    "\n",
    "# Develop\n",
    "DEVELOP = True\n",
    "\n",
    "# HYPERPARAMETER OPTIMISATION\n",
    "HYPEROPT = True\n",
    "\n",
    "if HYPEROPT:\n",
    "    OPTUNA = False # Optimise using Optuna\n",
    "    GRIDSEARCH = False  # Optimise using GridSearchCV\n",
    "    RANDOMSEARCH = True  # Optimise using RandomizedSearchCV\n",
    "\n",
    "# SAVE SELECTED FEATURES in root / models / selected_features\n",
    "SAVE_SELECT_FEATURES = True\n",
    "\n",
    "# WORKING SERVER\n",
    "AVAILABLE_SERVERS = ['ZS', 'PLOEN', 'KIEL', 'WYK']\n",
    "CURRENT_SERVER = AVAILABLE_SERVERS[1]\n",
    "\n",
    "# seed\n",
    "split_seed = 42\n",
    "\n",
    "# If DOWN_SAMPLE, define the target time resolution\n",
    "targeted_resolution_min = 1  # minute\n",
    "\n",
    "# TODO: The following featues are corrupted by containing NaNs. Fix this. For now, these columns are dropped\n",
    "corrupted_features = [\"stopped\", \"abs_ccs\", \"curv\"]\n",
    "\n",
    "\n",
    "# Use up to 70% of the available cpu cores\n",
    "n_jobs = joblib.cpu_count()\n",
    "print(\"Number of CPUs available:\", n_jobs)\n",
    "if CURRENT_SERVER == 'ZS':\n",
    "    n_jobs = int(0.9 * n_jobs)\n",
    "else:\n",
    "    n_jobs = int(0.7 * n_jobs)\n",
    "print(\"Number of CPUs to use:\", n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PATHS >>>\n",
    "# data dir\n",
    "data_dir = root_dir / 'data'\n",
    "data_dir = data_dir.resolve()\n",
    "if not data_dir.exists():\n",
    "    raise FileNotFoundError('Data directory not found')\n",
    "\n",
    "if CURRENT_SERVER == 'ZS':\n",
    "    # assets dir  # TODO: Used temporarly during the features seletion process. Remove this!\n",
    "    assets_dir = data_dir / 'assets'\n",
    "    assets_dir = assets_dir.resolve()\n",
    "    if not assets_dir.exists():\n",
    "        raise FileNotFoundError(f'Assets directory in {CURRENT_SERVER} not found')\n",
    "else:\n",
    "    # aistraj dir\n",
    "    assets_dir = data_dir / 'local' / 'aistraj'\n",
    "    assets_dir = assets_dir.resolve()\n",
    "    if not assets_dir.exists():\n",
    "        raise FileNotFoundError('Assets directory not found')\n",
    "\n",
    "    # train-validate-test (tvt) dir\n",
    "    tvt_assets_dir = assets_dir / 'tvt_assets'\n",
    "    tvt_assets_dir = tvt_assets_dir.resolve()\n",
    "    if not tvt_assets_dir.exists():\n",
    "        raise FileNotFoundError('Train-Validate-Test Assets directory not found')\n",
    "\n",
    "    # tvt: extended pickle dir\n",
    "    tvt_extended_dir = tvt_assets_dir / 'extended'\n",
    "    tvt_extended_dir = tvt_extended_dir.resolve()\n",
    "    if not tvt_extended_dir.exists():\n",
    "        raise FileNotFoundError('TVT Extended Pickled Data directory not found')\n",
    "\n",
    "    # tvt: scaled pickle dir\n",
    "    tvt_scaled_dir = tvt_assets_dir / 'scaled'\n",
    "    tvt_scaled_dir = tvt_scaled_dir.resolve()\n",
    "    if not tvt_scaled_dir.exists():\n",
    "        raise FileNotFoundError('TVT Scaled Pickled Data directory not found')\n",
    "\n",
    "    # tvt: logs dir\n",
    "    tvt_logs_dir = tvt_assets_dir / 'logs'\n",
    "    tvt_logs_dir = tvt_logs_dir.resolve()\n",
    "    if not tvt_logs_dir.exists():\n",
    "        raise FileNotFoundError('TVT logs directory not found')\n",
    "    \n",
    "# models dir\n",
    "models_dir = root_dir / 'models'\n",
    "models_dir = models_dir.resolve()\n",
    "if not models_dir.exists():\n",
    "    raise FileNotFoundError('Models directory not found')\n",
    "\n",
    "# Selected Features dir\n",
    "selected_features_dir = models_dir / 'selected_features'\n",
    "selected_features_dir = selected_features_dir.resolve()\n",
    "if not selected_features_dir.exists():\n",
    "    raise FileNotFoundError('selected features directory not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. <a id='toc1_2_'></a>[Load the data](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Select the paths of the scaled datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_paths = {'train': None, 'validate': None, 'test': None}\n",
    "\n",
    "if DOWN_SAMPLE:\n",
    "    import_paths = {\n",
    "                    'train': tvt_scaled_dir / 'scaled_cleaned_downsampled_extended_train_df.parquet',\n",
    "                    'validate': tvt_scaled_dir / 'scaled_cleaned_downsampled_extended_validate_df.parquet',\n",
    "                    'test': tvt_scaled_dir / 'scaled_cleaned_downsampled_extended_test_df.parquet'\n",
    "                    }\n",
    "else:  \n",
    "    if CURRENT_SERVER != 'ZS':\n",
    "        import_paths = {\n",
    "                        'train': tvt_scaled_dir / 'scaled_cleaned_extended_train_df.parquet',\n",
    "                        'validate': tvt_scaled_dir / 'scaled_cleaned_extended_validate_df.parquet',\n",
    "                        'test': tvt_scaled_dir / 'scaled_cleaned_extended_test_df.parquet'\n",
    "                        }\n",
    "    else:\n",
    "        import_paths = {\n",
    "                        'train': assets_dir / 'scaled_cleaned_extended_train_df.parquet',\n",
    "                        'validate': assets_dir / 'scaled_cleaned_extended_validate_df.parquet',\n",
    "                        'test': assets_dir / 'scaled_cleaned_extended_test_df.parquet'\n",
    "                        }\n",
    "        \n",
    "# Assets container >\n",
    "train_df, validate_df, test_df = None, None, None\n",
    "assets = {'train': train_df, 'validate': validate_df, 'test': test_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if not DEVELOP:  # Data is huge! don't use for exploring and developping\n",
    "#     train_df = load_df_to_dataset(data_path=import_paths['train'], use_dask=False).data  # Load the train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the validate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "validate_df = load_df_to_dataset(import_paths['validate'], use_dask=False).data  # Load the validate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    columns = validate_df.columns\n",
    "    print(f\"Num. Cols: {len(columns)}: {columns}\")\n",
    "    print()\n",
    "    print(f\"Num. Samples: {validate_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    display(validate_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    validate_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if not DEVELOP:  # Data is huge! don't use for exploring and developping\n",
    "#     test_df = load_df_to_dataset(import_paths['test'], use_dask=False).data  # Load the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Concat the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate the datasets >\n",
    "asset_df = validate_df  # asset_df = pd.concat([train_df, validate_df, test_df], axis=0)\n",
    "\n",
    "# # Sort the dataset by epoch >\n",
    "# asset_df = asset_df.sort_values(by='epoch', ascending=True)\n",
    "\n",
    "# # Reset the index >\n",
    "# asset_df = asset_df.reset_index(drop=True)\n",
    "\n",
    "# # Display the dataset's head >\n",
    "# if EXPLORE:\n",
    "#     asset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter-based features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_not_to_study = ['epoch', 'datetime', 'obj_id', 'traj_id', 'stopped', 'curv']\n",
    "\n",
    "# Check that the column in cols_not_to_study are in the dataset, otherwise remove them from the list >\n",
    "cols_not_to_study = [col for col in cols_not_to_study if col in asset_df.columns]\n",
    "\n",
    "print(f\"Cols not to study: {cols_not_to_study}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Regressor AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLPRegressor autoencoders provide a powerful framework for feature selection by leveraging the network's ability to learn meaningful representations of input data in a lower-dimensional space.\n",
    "\n",
    "Using an MLPRegressor (Multi-layer Perceptron Regressor) as an autoencoder for feature selection involves utilizing the network's ability to learn and compress input data into a lower-dimensional representation. This compression process serves as a form of feature selection because it forces the network to learn the most important features needed to reconstruct the input data.\n",
    "\n",
    "Here's how the process typically works:\n",
    "\n",
    "1. **Input Data**: You start with a dataset containing a set of features, possibly with many dimensions.\n",
    "\n",
    "2. **Autoencoder Architecture**: You define an MLPRegressor architecture with at least one hidden layer. The size of the hidden layer(s) is typically smaller than the input layer, which forces the network to learn a compressed representation of the input features.\n",
    "\n",
    "3. **Training**: The autoencoder is trained using the same input data for both the input and output layers. The objective is to minimize the reconstruction error, i.e., the difference between the input and the output. During training, the network learns to map the input data to a lower-dimensional space and then reconstruct it back to its original form.\n",
    "\n",
    "4. **Feature Importance**: After training, the hidden layers of the autoencoder contain representations of the input data. The activations of neurons in these hidden layers can be interpreted as feature importance scores. Features that contribute more to the reconstruction of the input data will have higher activations in the hidden layers.\n",
    "\n",
    "5. **Selection**: Based on the learned representations, you can select features by considering the activations of neurons in the hidden layers. Features corresponding to neurons with higher activations are considered more important, and those with lower activations are considered less important. You can either directly use these activations as feature scores or apply further processing (e.g., normalization) to obtain more interpretable scores.\n",
    "\n",
    "Using an MLPRegressor as an autoencoder for feature selection offers several advantages:\n",
    "\n",
    "- **Non-linearity**: MLPs can capture complex relationships between features, allowing for more effective feature selection compared to linear methods.\n",
    "- **Flexibility**: The architecture of the autoencoder can be customized based on the characteristics of the dataset, allowing for adaptability to different types of data.\n",
    "- **Unsupervised Learning**: Autoencoders do not require labeled data for training, making them suitable for unsupervised feature selection tasks where labeled data may be limited or unavailable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Optuna objective function for the optimisation of ``threshold`` hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create a copy of the dataset and drop the columns not to study >\n",
    "df = asset_df.drop(columns=cols_not_to_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Find the best threshold value for the variance threshold method using Optuna. Using the silhouette score with k-means clustering.\n",
    "  > **NOTE**:</br> Assuming that the number of clusters is $10$.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_autoencoder(df, n_hidden, activation='relu', solver='adam', max_iter=1000, random_state=split_seed) -> Tuple[MLPRegressor, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Perform feature selection using an MLPRegressor autoencoder.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe containing the features.\n",
    "        n_hidden (int): The number of hidden units in the autoencoder.\n",
    "        activation (str, optional): The activation function to use in the autoencoder. Defaults to 'relu'.\n",
    "        solver (str, optional): The solver to use in the autoencoder. Defaults to 'adam'.\n",
    "        max_iter (int, optional): The maximum number of iterations for training the autoencoder. Defaults to 1000.\n",
    "        random_state (int, optional): The random state for reproducibility. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists - selected_features and feature_scores.\n",
    "            - selected_features (list): The selected features based on their importance scores.\n",
    "            - feature_scores (list): The importance scores of the selected features.\n",
    "    \"\"\"\n",
    "    # Instantiate a place holder for the variance threshold method (vtm) selected features >\n",
    "    fs_df = pd.DataFrame(columns=['selected_features', 'feature_importance'])\n",
    "    \n",
    "    # Feature selection using MLPRegressor autoencoder\n",
    "    mlp_autoencoder = MLPRegressor(hidden_layer_sizes=(len(df.columns) // 2,), activation=activation, solver=solver, max_iter=max_iter, random_state=random_state)\n",
    "    mlp_autoencoder.fit(df, df)\n",
    "    encoded_features = mlp_autoencoder.predict(df)\n",
    "    \n",
    "    # Calculate reconstruction errors\n",
    "    reconstruction_errors = np.mean(np.square(df.values - encoded_features), axis=0).reshape(1, -1)  # Reshape to 2D array\n",
    "\n",
    "    # Normalize reconstruction errors using Normalizer\n",
    "    normalizer = Normalizer()\n",
    "    normalized_errors = normalizer.fit_transform(reconstruction_errors)\n",
    "    \n",
    "    # Extract normalized scores\n",
    "    feature_scores = 1 - normalized_errors.flatten()\n",
    "    \n",
    "    # Sort features based on importance scores\n",
    "    sorted_features = sorted(zip(df.columns, feature_scores), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Normalize the reconstruction errors to get scores that represent the importance of each feature. \n",
    "    selected_features = [feat for feat, _ in sorted_features]\n",
    "    feature_scores = [score for _, score in sorted_features]\n",
    "    \n",
    "    # put the data in fs_df >\n",
    "    fs_df['selected_features'] = selected_features\n",
    "    fs_df['feature_importance'] = feature_scores\n",
    "    \n",
    "    return mlp_autoencoder, fs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_mlp_hyperparameters(df):\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters for a MLPRegressor model using GridSearchCV.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The input DataFrame for training the model.\n",
    "\n",
    "    Returns:\n",
    "    - best_params: The best hyperparameters found by GridSearchCV.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hyperparameter optimization using GridSearchCV\n",
    "    param_grid = {\n",
    "        'mlp__hidden_layer_sizes': [(10,), (50,), (100,), (200,)],\n",
    "        'mlp__activation': ['relu', 'tanh'],\n",
    "        'mlp__solver': ['adam', 'lbfgs'],\n",
    "        'mlp__max_iter': [500, 1000, 1500]\n",
    "    }\n",
    "    \n",
    "    # Declare the MLP AE\n",
    "    mlp = MLPRegressor(random_state=split_seed)\n",
    "    # Declare the kMeans >\n",
    "    n_samples = df.shape[0]\n",
    "    kmeans = KMeans(n_clusters=min(30, n_samples//2), random_state=42)\n",
    "    # Create the pipeline >\n",
    "    pipeline = Pipeline([('mlp', mlp), ('kmeans', kmeans)])\n",
    "    \n",
    "    # Define a function to compute silhouette score\n",
    "    def silhouette_scorer(estimator, X):\n",
    "            labels = estimator.predict(X)\n",
    "            return silhouette_score(X, labels)\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=pipeline, \n",
    "                               param_grid=param_grid, \n",
    "                               cv=None, \n",
    "                               scoring=silhouette_scorer,  # 'neg_mean_squared_error',\n",
    "                               n_jobs=n_jobs,\n",
    "                               verbose=2)\n",
    "    grid_search.fit(df, df)\n",
    "    \n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "best_params = optimize_mlp_hyperparameters(df)\n",
    "\n",
    "best_n_hidden = best_params['mlp__hidden_layer_sizes'][0]\n",
    "best_activation = best_params['mlp__activation']\n",
    "best_solver = best_params['mlp__solver']\n",
    "best_max_iter = best_params['mlp__max_iter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, fs_df = feature_selection_autoencoder(df, best_n_hidden, best_activation, best_solver, best_max_iter)\n",
    "\n",
    "# Free up memory\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(fs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the selected features to the models directory >\n",
    "if SAVE_SELECT_FEATURES:\n",
    "    fs_df.to_csv(selected_features_dir / 'selected_features_mlp.csv', index=False)\n",
    "    print(\"Selected Features saved to:\", selected_features_dir / 'selected_features_mlp.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captn-nba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
